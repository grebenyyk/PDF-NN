{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337) \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, BatchNormalization, MaxPooling1D, LeakyReLU, Flatten, Dropout\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.utils import custom_object_scope\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import metrics, regularizers\n",
    "from keras.optimizers import Adam, Adagrad, Adadelta, RMSprop\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras.regularizers\n",
    "import keras\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/dimitrygrebenyuk/Yandex.Disk.localized/Working/PDF/Refinements/PDF-Cluster-Prediction/all_data')\n",
    "files_calc = glob.glob('*.dat')\n",
    "files_exp = glob.glob('*processed.gr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_points = []\n",
    "\n",
    "with open('labels.txt', 'w') as labels:\n",
    "    for f in files_calc:\n",
    "        df = pd.read_csv(f, usecols=[1], skiprows=201, header=None, delim_whitespace=True, skipfooter=0, engine='python')\n",
    "        raw_data_points.append(df.values.ravel())\n",
    "        labels.write(f[0])\n",
    "        labels.write('\\n')\n",
    "    for f in files_exp:\n",
    "        df = pd.read_csv(f, usecols=[1], skiprows=1, header=None, delim_whitespace=True, skipfooter=1, engine='python')\n",
    "        raw_data_points.append(df.values.ravel())\n",
    "        labels.write(f[0])\n",
    "        labels.write('\\n')\n",
    "        \n",
    "raw_data_points = np.array(raw_data_points)\n",
    "\n",
    "# Load the labels\n",
    "labels = pd.read_csv(\"labels.txt\", header=None)\n",
    "one_hot_labels = pd.get_dummies(labels[0])\n",
    "labels = one_hot_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2858959a0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Preprocessing\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "#scaler =MaxAbsScaler()\n",
    "#data_points = scaler.fit_transform(data_points)\n",
    "normalize = Normalizer()\n",
    "data_points = normalize.fit_transform(raw_data_points)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(data_points[43,:]))/100+2, data_points[43,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_points, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 745, 32)           8192      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 745, 32)          128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 372, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 372, 32)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 357, 32)           16416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 357, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 178, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 178, 32)           0         \n",
      "                                                                 \n",
      " seq_self_attention (SeqSelf  (None, 178, 32)          1025      \n",
      " Attention)                                                      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5696)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               729216    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 756,009\n",
      "Trainable params: 755,945\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tuning of the model\n",
    "\n",
    "model = Sequential()\n",
    "# Add the convolutional layers\n",
    "model.add(Conv1D(filters=32, kernel_size=256, activation='relu', input_shape=(1000,1), use_bias=False)) # 256, 32\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2)) # 2\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=16, activation='relu')) # 64, 32\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2)) # 2\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(SeqSelfAttention(attention_width=16, attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL)) # 16\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add the fully connected layers\n",
    "model.add(Dense(units=128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))) # 128 0.01\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=8, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.01))) # 10 0.01\n",
    "\n",
    "# Compile the model\n",
    "optimizer = RMSprop(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 14:17:10.545726: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 0s - loss: 4.7105 - accuracy: 0.2708\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 1s 98ms/step - loss: 4.6913 - accuracy: 0.2755 - val_loss: 4.6802 - val_accuracy: 0.3200\n",
      "Epoch 2/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 4.3377 - accuracy: 0.4286\n",
      "Epoch 2: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 4.3377 - accuracy: 0.4286 - val_loss: 4.6443 - val_accuracy: 0.3200\n",
      "Epoch 3/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.3813 - accuracy: 0.4062\n",
      "Epoch 3: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 4.3782 - accuracy: 0.4082 - val_loss: 4.6158 - val_accuracy: 0.3200\n",
      "Epoch 4/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.2435 - accuracy: 0.4271\n",
      "Epoch 4: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 4.2282 - accuracy: 0.4388 - val_loss: 4.5892 - val_accuracy: 0.3200\n",
      "Epoch 5/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.1193 - accuracy: 0.4271\n",
      "Epoch 5: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 4.1318 - accuracy: 0.4184 - val_loss: 4.5687 - val_accuracy: 0.3200\n",
      "Epoch 6/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.1425 - accuracy: 0.4167\n",
      "Epoch 6: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 4.1601 - accuracy: 0.4082 - val_loss: 4.5488 - val_accuracy: 0.3200\n",
      "Epoch 7/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.1270 - accuracy: 0.4792\n",
      "Epoch 7: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 4.1119 - accuracy: 0.4796 - val_loss: 4.5245 - val_accuracy: 0.3200\n",
      "Epoch 8/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 4.0063 - accuracy: 0.4479\n",
      "Epoch 8: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 3.9899 - accuracy: 0.4592 - val_loss: 4.5008 - val_accuracy: 0.3200\n",
      "Epoch 9/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.9302 - accuracy: 0.4688\n",
      "Epoch 9: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 3.9496 - accuracy: 0.4592 - val_loss: 4.4837 - val_accuracy: 0.3200\n",
      "Epoch 10/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.9722 - accuracy: 0.4479\n",
      "Epoch 10: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 3.9543 - accuracy: 0.4592 - val_loss: 4.4610 - val_accuracy: 0.3200\n",
      "Epoch 11/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.8505 - accuracy: 0.4479\n",
      "Epoch 11: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 3.8341 - accuracy: 0.4592 - val_loss: 4.4378 - val_accuracy: 0.3200\n",
      "Epoch 12/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.9130 - accuracy: 0.4792\n",
      "Epoch 12: val_accuracy did not improve from 0.32000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 3.9184 - accuracy: 0.4796 - val_loss: 4.4186 - val_accuracy: 0.3200\n",
      "Epoch 13/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.8901 - accuracy: 0.4896\n",
      "Epoch 13: val_accuracy improved from 0.32000 to 0.36000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 3.8995 - accuracy: 0.4796 - val_loss: 4.4015 - val_accuracy: 0.3600\n",
      "Epoch 14/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.7785 - accuracy: 0.5625\n",
      "Epoch 14: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.7631 - accuracy: 0.5714 - val_loss: 4.3805 - val_accuracy: 0.3600\n",
      "Epoch 15/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.7842 - accuracy: 0.4792\n",
      "Epoch 15: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 3.7617 - accuracy: 0.4898 - val_loss: 4.3565 - val_accuracy: 0.3200\n",
      "Epoch 16/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.7954 - accuracy: 0.5104\n",
      "Epoch 16: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 3.7691 - accuracy: 0.5204 - val_loss: 4.3385 - val_accuracy: 0.3200\n",
      "Epoch 17/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.5716 - accuracy: 0.5729\n",
      "Epoch 17: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 3.5687 - accuracy: 0.5714 - val_loss: 4.3180 - val_accuracy: 0.3200\n",
      "Epoch 18/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.5089 - accuracy: 0.5938\n",
      "Epoch 18: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 3.5125 - accuracy: 0.5918 - val_loss: 4.3000 - val_accuracy: 0.3200\n",
      "Epoch 19/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.4515 - accuracy: 0.6354\n",
      "Epoch 19: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 3.4696 - accuracy: 0.6224 - val_loss: 4.2768 - val_accuracy: 0.3200\n",
      "Epoch 20/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.4941 - accuracy: 0.5729\n",
      "Epoch 20: val_accuracy did not improve from 0.36000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 3.5206 - accuracy: 0.5714 - val_loss: 4.2614 - val_accuracy: 0.3200\n",
      "Epoch 21/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 3.5732 - accuracy: 0.5306\n",
      "Epoch 21: val_accuracy improved from 0.36000 to 0.40000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 3.5732 - accuracy: 0.5306 - val_loss: 4.2455 - val_accuracy: 0.4000\n",
      "Epoch 22/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.4511 - accuracy: 0.5729\n",
      "Epoch 22: val_accuracy improved from 0.40000 to 0.44000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 3.4562 - accuracy: 0.5714 - val_loss: 4.2271 - val_accuracy: 0.4400\n",
      "Epoch 23/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2972 - accuracy: 0.6354\n",
      "Epoch 23: val_accuracy did not improve from 0.44000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.2920 - accuracy: 0.6429 - val_loss: 4.2067 - val_accuracy: 0.3200\n",
      "Epoch 24/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.4688 - accuracy: 0.5521\n",
      "Epoch 24: val_accuracy did not improve from 0.44000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.4860 - accuracy: 0.5510 - val_loss: 4.1911 - val_accuracy: 0.4400\n",
      "Epoch 25/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.3586 - accuracy: 0.5729\n",
      "Epoch 25: val_accuracy did not improve from 0.44000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.3595 - accuracy: 0.5714 - val_loss: 4.1746 - val_accuracy: 0.3600\n",
      "Epoch 26/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.3232 - accuracy: 0.5625\n",
      "Epoch 26: val_accuracy did not improve from 0.44000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 3.3114 - accuracy: 0.5612 - val_loss: 4.1566 - val_accuracy: 0.4000\n",
      "Epoch 27/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.3253 - accuracy: 0.5521\n",
      "Epoch 27: val_accuracy did not improve from 0.44000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.3242 - accuracy: 0.5510 - val_loss: 4.1378 - val_accuracy: 0.3600\n",
      "Epoch 28/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2708 - accuracy: 0.6146\n",
      "Epoch 28: val_accuracy improved from 0.44000 to 0.60000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 3.3067 - accuracy: 0.6020 - val_loss: 4.1266 - val_accuracy: 0.6000\n",
      "Epoch 29/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2461 - accuracy: 0.5625\n",
      "Epoch 29: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.2354 - accuracy: 0.5612 - val_loss: 4.1054 - val_accuracy: 0.4000\n",
      "Epoch 30/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2983 - accuracy: 0.5938\n",
      "Epoch 30: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.2830 - accuracy: 0.6020 - val_loss: 4.0868 - val_accuracy: 0.4800\n",
      "Epoch 31/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.1744 - accuracy: 0.6458\n",
      "Epoch 31: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.1640 - accuracy: 0.6531 - val_loss: 4.0656 - val_accuracy: 0.3600\n",
      "Epoch 32/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.1060 - accuracy: 0.6250\n",
      "Epoch 32: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.1158 - accuracy: 0.6224 - val_loss: 4.0495 - val_accuracy: 0.3600\n",
      "Epoch 33/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2176 - accuracy: 0.5729\n",
      "Epoch 33: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.2044 - accuracy: 0.5816 - val_loss: 4.0344 - val_accuracy: 0.4000\n",
      "Epoch 34/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.1448 - accuracy: 0.5833\n",
      "Epoch 34: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 3.1450 - accuracy: 0.5816 - val_loss: 4.0247 - val_accuracy: 0.6000\n",
      "Epoch 35/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.2265 - accuracy: 0.5729\n",
      "Epoch 35: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.2327 - accuracy: 0.5714 - val_loss: 4.0122 - val_accuracy: 0.6000\n",
      "Epoch 36/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.0728 - accuracy: 0.6458\n",
      "Epoch 36: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.0739 - accuracy: 0.6429 - val_loss: 3.9899 - val_accuracy: 0.4400\n",
      "Epoch 37/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.0849 - accuracy: 0.6354\n",
      "Epoch 37: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.0649 - accuracy: 0.6429 - val_loss: 3.9727 - val_accuracy: 0.4400\n",
      "Epoch 38/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.0271 - accuracy: 0.6458\n",
      "Epoch 38: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.0141 - accuracy: 0.6531 - val_loss: 3.9513 - val_accuracy: 0.4000\n",
      "Epoch 39/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.9723 - accuracy: 0.6667\n",
      "Epoch 39: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.9669 - accuracy: 0.6633 - val_loss: 3.9455 - val_accuracy: 0.4400\n",
      "Epoch 40/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.9318 - accuracy: 0.6771\n",
      "Epoch 40: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.9170 - accuracy: 0.6837 - val_loss: 3.9237 - val_accuracy: 0.4000\n",
      "Epoch 41/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.0064 - accuracy: 0.6354\n",
      "Epoch 41: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0172 - accuracy: 0.6327 - val_loss: 3.9096 - val_accuracy: 0.4400\n",
      "Epoch 42/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 3.0414 - accuracy: 0.6354\n",
      "Epoch 42: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.0201 - accuracy: 0.6429 - val_loss: 3.8960 - val_accuracy: 0.4000\n",
      "Epoch 43/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.8382 - accuracy: 0.6875\n",
      "Epoch 43: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.8519 - accuracy: 0.6837 - val_loss: 3.8887 - val_accuracy: 0.4000\n",
      "Epoch 44/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.9012 - accuracy: 0.6042\n",
      "Epoch 44: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.8940 - accuracy: 0.6020 - val_loss: 3.8685 - val_accuracy: 0.5200\n",
      "Epoch 45/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.9226 - accuracy: 0.6250\n",
      "Epoch 45: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.9205 - accuracy: 0.6224 - val_loss: 3.8498 - val_accuracy: 0.4000\n",
      "Epoch 46/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.8464 - accuracy: 0.6771\n",
      "Epoch 46: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.8360 - accuracy: 0.6837 - val_loss: 3.8300 - val_accuracy: 0.4400\n",
      "Epoch 47/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.9237 - accuracy: 0.6429\n",
      "Epoch 47: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.9237 - accuracy: 0.6429 - val_loss: 3.8204 - val_accuracy: 0.4400\n",
      "Epoch 48/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.8167 - accuracy: 0.7083\n",
      "Epoch 48: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.8008 - accuracy: 0.7143 - val_loss: 3.8045 - val_accuracy: 0.4400\n",
      "Epoch 49/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.7318 - accuracy: 0.7292\n",
      "Epoch 49: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.7396 - accuracy: 0.7245 - val_loss: 3.7893 - val_accuracy: 0.5600\n",
      "Epoch 50/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.7903 - accuracy: 0.6667\n",
      "Epoch 50: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.7850 - accuracy: 0.6633 - val_loss: 3.7761 - val_accuracy: 0.5600\n",
      "Epoch 51/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.7400 - accuracy: 0.6667\n",
      "Epoch 51: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.7291 - accuracy: 0.6735 - val_loss: 3.7533 - val_accuracy: 0.4800\n",
      "Epoch 52/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.7181 - accuracy: 0.7083\n",
      "Epoch 52: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.7125 - accuracy: 0.7041 - val_loss: 3.7514 - val_accuracy: 0.4800\n",
      "Epoch 53/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.6617 - accuracy: 0.6667\n",
      "Epoch 53: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.6872 - accuracy: 0.6531 - val_loss: 3.7400 - val_accuracy: 0.5600\n",
      "Epoch 54/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5688 - accuracy: 0.7604\n",
      "Epoch 54: val_accuracy did not improve from 0.60000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.5989 - accuracy: 0.7449 - val_loss: 3.7287 - val_accuracy: 0.6000\n",
      "Epoch 55/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.6492 - accuracy: 0.6771\n",
      "Epoch 55: val_accuracy improved from 0.60000 to 0.64000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.6595 - accuracy: 0.6735 - val_loss: 3.7095 - val_accuracy: 0.6400\n",
      "Epoch 56/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5549 - accuracy: 0.7500\n",
      "Epoch 56: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.5610 - accuracy: 0.7449 - val_loss: 3.7041 - val_accuracy: 0.6000\n",
      "Epoch 57/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.6550 - accuracy: 0.7083\n",
      "Epoch 57: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.6661 - accuracy: 0.7041 - val_loss: 3.6892 - val_accuracy: 0.5200\n",
      "Epoch 58/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.6597 - accuracy: 0.6875\n",
      "Epoch 58: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.6505 - accuracy: 0.6939 - val_loss: 3.6788 - val_accuracy: 0.5600\n",
      "Epoch 59/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5628 - accuracy: 0.7083\n",
      "Epoch 59: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 2.5475 - accuracy: 0.7143 - val_loss: 3.6661 - val_accuracy: 0.6400\n",
      "Epoch 60/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.5624 - accuracy: 0.6939\n",
      "Epoch 60: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.5624 - accuracy: 0.6939 - val_loss: 3.6595 - val_accuracy: 0.5600\n",
      "Epoch 61/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5354 - accuracy: 0.6875\n",
      "Epoch 61: val_accuracy did not improve from 0.64000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.5557 - accuracy: 0.6837 - val_loss: 3.6434 - val_accuracy: 0.6000\n",
      "Epoch 62/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5097 - accuracy: 0.7188\n",
      "Epoch 62: val_accuracy improved from 0.64000 to 0.76000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.5479 - accuracy: 0.7143 - val_loss: 3.6390 - val_accuracy: 0.7600\n",
      "Epoch 63/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.6688 - accuracy: 0.6458\n",
      "Epoch 63: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.6668 - accuracy: 0.6429 - val_loss: 3.6264 - val_accuracy: 0.5600\n",
      "Epoch 64/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.6031 - accuracy: 0.6837\n",
      "Epoch 64: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.6031 - accuracy: 0.6837 - val_loss: 3.6166 - val_accuracy: 0.4800\n",
      "Epoch 65/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5769 - accuracy: 0.6667\n",
      "Epoch 65: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.5639 - accuracy: 0.6735 - val_loss: 3.5978 - val_accuracy: 0.5600\n",
      "Epoch 66/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4806 - accuracy: 0.7292\n",
      "Epoch 66: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.4653 - accuracy: 0.7347 - val_loss: 3.5822 - val_accuracy: 0.6400\n",
      "Epoch 67/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4022 - accuracy: 0.7500\n",
      "Epoch 67: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.4133 - accuracy: 0.7449 - val_loss: 3.5672 - val_accuracy: 0.6000\n",
      "Epoch 68/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.4964 - accuracy: 0.7041\n",
      "Epoch 68: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.4964 - accuracy: 0.7041 - val_loss: 3.5638 - val_accuracy: 0.5200\n",
      "Epoch 69/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4535 - accuracy: 0.6771\n",
      "Epoch 69: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 2.4406 - accuracy: 0.6837 - val_loss: 3.5481 - val_accuracy: 0.6400\n",
      "Epoch 70/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5273 - accuracy: 0.7188\n",
      "Epoch 70: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.5359 - accuracy: 0.7143 - val_loss: 3.5404 - val_accuracy: 0.6400\n",
      "Epoch 71/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.5527 - accuracy: 0.6667\n",
      "Epoch 71: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.5383 - accuracy: 0.6735 - val_loss: 3.5292 - val_accuracy: 0.6000\n",
      "Epoch 72/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.4226 - accuracy: 0.7653\n",
      "Epoch 72: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.4226 - accuracy: 0.7653 - val_loss: 3.5237 - val_accuracy: 0.6800\n",
      "Epoch 73/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4432 - accuracy: 0.7188\n",
      "Epoch 73: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.4385 - accuracy: 0.7245 - val_loss: 3.5030 - val_accuracy: 0.6800\n",
      "Epoch 74/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4515 - accuracy: 0.7083\n",
      "Epoch 74: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.4492 - accuracy: 0.7041 - val_loss: 3.4946 - val_accuracy: 0.6800\n",
      "Epoch 75/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3359 - accuracy: 0.7292\n",
      "Epoch 75: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.3365 - accuracy: 0.7347 - val_loss: 3.4762 - val_accuracy: 0.5600\n",
      "Epoch 76/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3718 - accuracy: 0.6875\n",
      "Epoch 76: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.3741 - accuracy: 0.6837 - val_loss: 3.4678 - val_accuracy: 0.7600\n",
      "Epoch 77/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.4872 - accuracy: 0.6667\n",
      "Epoch 77: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.4714 - accuracy: 0.6735 - val_loss: 3.4510 - val_accuracy: 0.6000\n",
      "Epoch 78/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.2520 - accuracy: 0.7812\n",
      "Epoch 78: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.2398 - accuracy: 0.7857 - val_loss: 3.4329 - val_accuracy: 0.6400\n",
      "Epoch 79/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.3106 - accuracy: 0.7551\n",
      "Epoch 79: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.3106 - accuracy: 0.7551 - val_loss: 3.4227 - val_accuracy: 0.6800\n",
      "Epoch 80/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3375 - accuracy: 0.7500\n",
      "Epoch 80: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.3470 - accuracy: 0.7449 - val_loss: 3.4069 - val_accuracy: 0.6400\n",
      "Epoch 81/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3557 - accuracy: 0.6979\n",
      "Epoch 81: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.3471 - accuracy: 0.7041 - val_loss: 3.3931 - val_accuracy: 0.6400\n",
      "Epoch 82/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3069 - accuracy: 0.7396\n",
      "Epoch 82: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.3057 - accuracy: 0.7347 - val_loss: 3.4009 - val_accuracy: 0.7600\n",
      "Epoch 83/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3009 - accuracy: 0.7604\n",
      "Epoch 83: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.3142 - accuracy: 0.7551 - val_loss: 3.3753 - val_accuracy: 0.6400\n",
      "Epoch 84/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1515 - accuracy: 0.7500\n",
      "Epoch 84: val_accuracy did not improve from 0.76000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.1521 - accuracy: 0.7551 - val_loss: 3.3705 - val_accuracy: 0.6400\n",
      "Epoch 85/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1931 - accuracy: 0.8125\n",
      "Epoch 85: val_accuracy improved from 0.76000 to 0.80000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.2180 - accuracy: 0.7959 - val_loss: 3.3632 - val_accuracy: 0.8000\n",
      "Epoch 86/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.3408 - accuracy: 0.6939\n",
      "Epoch 86: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.3408 - accuracy: 0.6939 - val_loss: 3.3462 - val_accuracy: 0.7600\n",
      "Epoch 87/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.2503 - accuracy: 0.7604\n",
      "Epoch 87: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.2444 - accuracy: 0.7653 - val_loss: 3.3253 - val_accuracy: 0.6400\n",
      "Epoch 88/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.1571 - accuracy: 0.7245\n",
      "Epoch 88: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.1571 - accuracy: 0.7245 - val_loss: 3.3206 - val_accuracy: 0.6800\n",
      "Epoch 89/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.2226 - accuracy: 0.7500\n",
      "Epoch 89: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 2.2131 - accuracy: 0.7551 - val_loss: 3.3161 - val_accuracy: 0.7200\n",
      "Epoch 90/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1094 - accuracy: 0.7812\n",
      "Epoch 90: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 2.1125 - accuracy: 0.7755 - val_loss: 3.3049 - val_accuracy: 0.6800\n",
      "Epoch 91/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1930 - accuracy: 0.7500\n",
      "Epoch 91: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.1852 - accuracy: 0.7551 - val_loss: 3.2851 - val_accuracy: 0.7200\n",
      "Epoch 92/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1669 - accuracy: 0.7500\n",
      "Epoch 92: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.1542 - accuracy: 0.7551 - val_loss: 3.2642 - val_accuracy: 0.6800\n",
      "Epoch 93/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.2174 - accuracy: 0.6939\n",
      "Epoch 93: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.2174 - accuracy: 0.6939 - val_loss: 3.2594 - val_accuracy: 0.7200\n",
      "Epoch 94/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1095 - accuracy: 0.7500\n",
      "Epoch 94: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.1157 - accuracy: 0.7449 - val_loss: 3.2319 - val_accuracy: 0.6800\n",
      "Epoch 95/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0809 - accuracy: 0.7812\n",
      "Epoch 95: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0933 - accuracy: 0.7755 - val_loss: 3.2120 - val_accuracy: 0.6800\n",
      "Epoch 96/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1041 - accuracy: 0.8021\n",
      "Epoch 96: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.0991 - accuracy: 0.8061 - val_loss: 3.1922 - val_accuracy: 0.6000\n",
      "Epoch 97/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0774 - accuracy: 0.7292\n",
      "Epoch 97: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0700 - accuracy: 0.7347 - val_loss: 3.1657 - val_accuracy: 0.6400\n",
      "Epoch 98/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1420 - accuracy: 0.7292\n",
      "Epoch 98: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.1340 - accuracy: 0.7347 - val_loss: 3.1652 - val_accuracy: 0.6400\n",
      "Epoch 99/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0816 - accuracy: 0.7812\n",
      "Epoch 99: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0855 - accuracy: 0.7755 - val_loss: 3.1736 - val_accuracy: 0.7600\n",
      "Epoch 100/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0135 - accuracy: 0.7812\n",
      "Epoch 100: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0111 - accuracy: 0.7755 - val_loss: 3.1522 - val_accuracy: 0.7600\n",
      "Epoch 101/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0425 - accuracy: 0.8021\n",
      "Epoch 101: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.0459 - accuracy: 0.7959 - val_loss: 3.1390 - val_accuracy: 0.8000\n",
      "Epoch 102/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0735 - accuracy: 0.7083\n",
      "Epoch 102: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0607 - accuracy: 0.7143 - val_loss: 3.1106 - val_accuracy: 0.7600\n",
      "Epoch 103/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0056 - accuracy: 0.7917\n",
      "Epoch 103: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.9972 - accuracy: 0.7959 - val_loss: 3.1063 - val_accuracy: 0.7600\n",
      "Epoch 104/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.9861 - accuracy: 0.7812\n",
      "Epoch 104: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.9888 - accuracy: 0.7755 - val_loss: 3.1070 - val_accuracy: 0.7600\n",
      "Epoch 105/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0362 - accuracy: 0.7917\n",
      "Epoch 105: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.0332 - accuracy: 0.7959 - val_loss: 3.0779 - val_accuracy: 0.7200\n",
      "Epoch 106/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0705 - accuracy: 0.7500\n",
      "Epoch 106: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 2.0735 - accuracy: 0.7449 - val_loss: 3.0738 - val_accuracy: 0.7600\n",
      "Epoch 107/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.9790 - accuracy: 0.7708\n",
      "Epoch 107: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0098 - accuracy: 0.7551 - val_loss: 3.0551 - val_accuracy: 0.6800\n",
      "Epoch 108/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0573 - accuracy: 0.7604\n",
      "Epoch 108: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0485 - accuracy: 0.7653 - val_loss: 3.0317 - val_accuracy: 0.6800\n",
      "Epoch 109/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.1123 - accuracy: 0.7188\n",
      "Epoch 109: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 2.1001 - accuracy: 0.7245 - val_loss: 3.0129 - val_accuracy: 0.7600\n",
      "Epoch 110/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0691 - accuracy: 0.7292\n",
      "Epoch 110: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.0735 - accuracy: 0.7245 - val_loss: 3.0103 - val_accuracy: 0.7600\n",
      "Epoch 111/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0269 - accuracy: 0.7188\n",
      "Epoch 111: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.0542 - accuracy: 0.7143 - val_loss: 3.0347 - val_accuracy: 0.8000\n",
      "Epoch 112/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0587 - accuracy: 0.7292\n",
      "Epoch 112: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 2.0485 - accuracy: 0.7347 - val_loss: 3.0011 - val_accuracy: 0.8000\n",
      "Epoch 113/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.9767 - accuracy: 0.7500\n",
      "Epoch 113: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.9785 - accuracy: 0.7551 - val_loss: 2.9739 - val_accuracy: 0.7600\n",
      "Epoch 114/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8932 - accuracy: 0.8438\n",
      "Epoch 114: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.9025 - accuracy: 0.8367 - val_loss: 2.9524 - val_accuracy: 0.7600\n",
      "Epoch 115/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.9689 - accuracy: 0.7449\n",
      "Epoch 115: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.9689 - accuracy: 0.7449 - val_loss: 2.9404 - val_accuracy: 0.8000\n",
      "Epoch 116/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8912 - accuracy: 0.7812\n",
      "Epoch 116: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.9038 - accuracy: 0.7755 - val_loss: 2.9359 - val_accuracy: 0.8000\n",
      "Epoch 117/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.0115 - accuracy: 0.7396\n",
      "Epoch 117: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.9978 - accuracy: 0.7449 - val_loss: 2.9114 - val_accuracy: 0.8000\n",
      "Epoch 118/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8594 - accuracy: 0.8333\n",
      "Epoch 118: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.8484 - accuracy: 0.8367 - val_loss: 2.8854 - val_accuracy: 0.7600\n",
      "Epoch 119/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7565 - accuracy: 0.8229\n",
      "Epoch 119: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.7603 - accuracy: 0.8163 - val_loss: 2.8725 - val_accuracy: 0.7600\n",
      "Epoch 120/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7234 - accuracy: 0.8438\n",
      "Epoch 120: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.7204 - accuracy: 0.8469 - val_loss: 2.8580 - val_accuracy: 0.7600\n",
      "Epoch 121/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.9282 - accuracy: 0.8021\n",
      "Epoch 121: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.9211 - accuracy: 0.8061 - val_loss: 2.8496 - val_accuracy: 0.6800\n",
      "Epoch 122/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8982 - accuracy: 0.8229\n",
      "Epoch 122: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.8875 - accuracy: 0.8265 - val_loss: 2.8415 - val_accuracy: 0.7600\n",
      "Epoch 123/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7780 - accuracy: 0.8542\n",
      "Epoch 123: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.7821 - accuracy: 0.8469 - val_loss: 2.7796 - val_accuracy: 0.6800\n",
      "Epoch 124/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.8542 - accuracy: 0.7959\n",
      "Epoch 124: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.8542 - accuracy: 0.7959 - val_loss: 2.7835 - val_accuracy: 0.8000\n",
      "Epoch 125/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8283 - accuracy: 0.7917\n",
      "Epoch 125: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.8450 - accuracy: 0.7857 - val_loss: 2.7922 - val_accuracy: 0.7200\n",
      "Epoch 126/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.8124 - accuracy: 0.7959\n",
      "Epoch 126: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.8124 - accuracy: 0.7959 - val_loss: 2.7626 - val_accuracy: 0.7200\n",
      "Epoch 127/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7993 - accuracy: 0.7812\n",
      "Epoch 127: val_accuracy did not improve from 0.80000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.7952 - accuracy: 0.7857 - val_loss: 2.7419 - val_accuracy: 0.7200\n",
      "Epoch 128/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.8836 - accuracy: 0.8061\n",
      "Epoch 128: val_accuracy improved from 0.80000 to 0.84000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 1.8836 - accuracy: 0.8061 - val_loss: 2.7198 - val_accuracy: 0.8400\n",
      "Epoch 129/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8614 - accuracy: 0.7708\n",
      "Epoch 129: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.8530 - accuracy: 0.7755 - val_loss: 2.6862 - val_accuracy: 0.6800\n",
      "Epoch 130/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7944 - accuracy: 0.7812\n",
      "Epoch 130: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.7834 - accuracy: 0.7857 - val_loss: 2.6801 - val_accuracy: 0.7600\n",
      "Epoch 131/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7473 - accuracy: 0.7917\n",
      "Epoch 131: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.7404 - accuracy: 0.7959 - val_loss: 2.6665 - val_accuracy: 0.8000\n",
      "Epoch 132/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7336 - accuracy: 0.8333\n",
      "Epoch 132: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.7279 - accuracy: 0.8367 - val_loss: 2.6305 - val_accuracy: 0.7600\n",
      "Epoch 133/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7494 - accuracy: 0.8229\n",
      "Epoch 133: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.7487 - accuracy: 0.8163 - val_loss: 2.6219 - val_accuracy: 0.6800\n",
      "Epoch 134/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.6071 - accuracy: 0.8776\n",
      "Epoch 134: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.6071 - accuracy: 0.8776 - val_loss: 2.5916 - val_accuracy: 0.6400\n",
      "Epoch 135/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7441 - accuracy: 0.8229\n",
      "Epoch 135: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.7568 - accuracy: 0.8163 - val_loss: 2.6158 - val_accuracy: 0.7200\n",
      "Epoch 136/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7349 - accuracy: 0.7917\n",
      "Epoch 136: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.7696 - accuracy: 0.7857 - val_loss: 2.6063 - val_accuracy: 0.8400\n",
      "Epoch 137/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.8711 - accuracy: 0.7500\n",
      "Epoch 137: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 1.8585 - accuracy: 0.7551 - val_loss: 2.5928 - val_accuracy: 0.8000\n",
      "Epoch 138/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7076 - accuracy: 0.8438\n",
      "Epoch 138: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.7083 - accuracy: 0.8469 - val_loss: 2.5951 - val_accuracy: 0.8000\n",
      "Epoch 139/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7338 - accuracy: 0.8125\n",
      "Epoch 139: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 1.7285 - accuracy: 0.8163 - val_loss: 2.5730 - val_accuracy: 0.8000\n",
      "Epoch 140/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.8179 - accuracy: 0.7551\n",
      "Epoch 140: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.8179 - accuracy: 0.7551 - val_loss: 2.5823 - val_accuracy: 0.8000\n",
      "Epoch 141/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6450 - accuracy: 0.8646\n",
      "Epoch 141: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 1.6394 - accuracy: 0.8673 - val_loss: 2.5469 - val_accuracy: 0.7600\n",
      "Epoch 142/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7033 - accuracy: 0.8333\n",
      "Epoch 142: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.6936 - accuracy: 0.8367 - val_loss: 2.5226 - val_accuracy: 0.8000\n",
      "Epoch 143/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6857 - accuracy: 0.8125\n",
      "Epoch 143: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.6755 - accuracy: 0.8163 - val_loss: 2.5154 - val_accuracy: 0.8000\n",
      "Epoch 144/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7035 - accuracy: 0.8229\n",
      "Epoch 144: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.7078 - accuracy: 0.8163 - val_loss: 2.4750 - val_accuracy: 0.7600\n",
      "Epoch 145/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5932 - accuracy: 0.8750\n",
      "Epoch 145: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.6033 - accuracy: 0.8673 - val_loss: 2.4544 - val_accuracy: 0.8000\n",
      "Epoch 146/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5573 - accuracy: 0.8542\n",
      "Epoch 146: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.5711 - accuracy: 0.8469 - val_loss: 2.4293 - val_accuracy: 0.7600\n",
      "Epoch 147/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7114 - accuracy: 0.8021\n",
      "Epoch 147: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.7203 - accuracy: 0.7959 - val_loss: 2.4284 - val_accuracy: 0.7600\n",
      "Epoch 148/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.7167 - accuracy: 0.7917\n",
      "Epoch 148: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.7054 - accuracy: 0.7959 - val_loss: 2.4091 - val_accuracy: 0.8400\n",
      "Epoch 149/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5768 - accuracy: 0.8542\n",
      "Epoch 149: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.5684 - accuracy: 0.8571 - val_loss: 2.3876 - val_accuracy: 0.8000\n",
      "Epoch 150/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5627 - accuracy: 0.8646\n",
      "Epoch 150: val_accuracy did not improve from 0.84000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.5592 - accuracy: 0.8673 - val_loss: 2.3957 - val_accuracy: 0.8000\n",
      "Epoch 151/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.6185 - accuracy: 0.8571\n",
      "Epoch 151: val_accuracy improved from 0.84000 to 0.88000, saving model to onehot.hdf5\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.6185 - accuracy: 0.8571 - val_loss: 2.4100 - val_accuracy: 0.8800\n",
      "Epoch 152/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6330 - accuracy: 0.8542\n",
      "Epoch 152: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.6231 - accuracy: 0.8571 - val_loss: 2.3758 - val_accuracy: 0.8400\n",
      "Epoch 153/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6080 - accuracy: 0.8750\n",
      "Epoch 153: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 1.6079 - accuracy: 0.8673 - val_loss: 2.3458 - val_accuracy: 0.8400\n",
      "Epoch 154/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5522 - accuracy: 0.8750\n",
      "Epoch 154: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.5458 - accuracy: 0.8776 - val_loss: 2.3112 - val_accuracy: 0.8400\n",
      "Epoch 155/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5640 - accuracy: 0.8438\n",
      "Epoch 155: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.5605 - accuracy: 0.8469 - val_loss: 2.2753 - val_accuracy: 0.8000\n",
      "Epoch 156/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4903 - accuracy: 0.8750\n",
      "Epoch 156: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4920 - accuracy: 0.8776 - val_loss: 2.2959 - val_accuracy: 0.7600\n",
      "Epoch 157/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.5540 - accuracy: 0.8469\n",
      "Epoch 157: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.5540 - accuracy: 0.8469 - val_loss: 2.2726 - val_accuracy: 0.7200\n",
      "Epoch 158/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5497 - accuracy: 0.8438\n",
      "Epoch 158: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5606 - accuracy: 0.8367 - val_loss: 2.3031 - val_accuracy: 0.8000\n",
      "Epoch 159/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5788 - accuracy: 0.8542\n",
      "Epoch 159: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.5774 - accuracy: 0.8571 - val_loss: 2.2618 - val_accuracy: 0.7200\n",
      "Epoch 160/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5158 - accuracy: 0.8542\n",
      "Epoch 160: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.5165 - accuracy: 0.8571 - val_loss: 2.2470 - val_accuracy: 0.7600\n",
      "Epoch 161/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6114 - accuracy: 0.7812\n",
      "Epoch 161: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.6014 - accuracy: 0.7857 - val_loss: 2.2219 - val_accuracy: 0.7600\n",
      "Epoch 162/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5113 - accuracy: 0.8646\n",
      "Epoch 162: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5124 - accuracy: 0.8673 - val_loss: 2.2036 - val_accuracy: 0.8000\n",
      "Epoch 163/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5601 - accuracy: 0.8333\n",
      "Epoch 163: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5513 - accuracy: 0.8367 - val_loss: 2.1824 - val_accuracy: 0.8000\n",
      "Epoch 164/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4575 - accuracy: 0.8750\n",
      "Epoch 164: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4536 - accuracy: 0.8776 - val_loss: 2.1969 - val_accuracy: 0.8000\n",
      "Epoch 165/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6049 - accuracy: 0.8021\n",
      "Epoch 165: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5963 - accuracy: 0.8061 - val_loss: 2.1735 - val_accuracy: 0.8000\n",
      "Epoch 166/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4406 - accuracy: 0.8542\n",
      "Epoch 166: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.4374 - accuracy: 0.8571 - val_loss: 2.1795 - val_accuracy: 0.8000\n",
      "Epoch 167/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4822 - accuracy: 0.8854\n",
      "Epoch 167: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.4975 - accuracy: 0.8776 - val_loss: 2.1467 - val_accuracy: 0.8000\n",
      "Epoch 168/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4088 - accuracy: 0.8646\n",
      "Epoch 168: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.4056 - accuracy: 0.8673 - val_loss: 2.1195 - val_accuracy: 0.8000\n",
      "Epoch 169/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5178 - accuracy: 0.8333\n",
      "Epoch 169: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.5156 - accuracy: 0.8367 - val_loss: 2.1135 - val_accuracy: 0.7600\n",
      "Epoch 170/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5303 - accuracy: 0.8438\n",
      "Epoch 170: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5215 - accuracy: 0.8469 - val_loss: 2.0544 - val_accuracy: 0.8000\n",
      "Epoch 171/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4748 - accuracy: 0.8438\n",
      "Epoch 171: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4723 - accuracy: 0.8469 - val_loss: 2.0685 - val_accuracy: 0.7600\n",
      "Epoch 172/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5605 - accuracy: 0.8125\n",
      "Epoch 172: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.5517 - accuracy: 0.8163 - val_loss: 2.0641 - val_accuracy: 0.8000\n",
      "Epoch 173/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4288 - accuracy: 0.8958\n",
      "Epoch 173: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4371 - accuracy: 0.8878 - val_loss: 2.0956 - val_accuracy: 0.7600\n",
      "Epoch 174/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4288 - accuracy: 0.8646\n",
      "Epoch 174: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.4230 - accuracy: 0.8673 - val_loss: 2.0137 - val_accuracy: 0.8000\n",
      "Epoch 175/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4451 - accuracy: 0.8125\n",
      "Epoch 175: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 1.4437 - accuracy: 0.8163 - val_loss: 2.0178 - val_accuracy: 0.8000\n",
      "Epoch 176/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3498 - accuracy: 0.9271\n",
      "Epoch 176: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.3583 - accuracy: 0.9184 - val_loss: 1.9797 - val_accuracy: 0.7600\n",
      "Epoch 177/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3128 - accuracy: 0.8958\n",
      "Epoch 177: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 1.3153 - accuracy: 0.8878 - val_loss: 1.9763 - val_accuracy: 0.8000\n",
      "Epoch 178/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.5074 - accuracy: 0.7812\n",
      "Epoch 178: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.5018 - accuracy: 0.7857 - val_loss: 1.9748 - val_accuracy: 0.7600\n",
      "Epoch 179/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3103 - accuracy: 0.9271\n",
      "Epoch 179: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.3060 - accuracy: 0.9286 - val_loss: 1.9304 - val_accuracy: 0.8000\n",
      "Epoch 180/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3816 - accuracy: 0.8438\n",
      "Epoch 180: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.3743 - accuracy: 0.8469 - val_loss: 1.9507 - val_accuracy: 0.8000\n",
      "Epoch 181/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4015 - accuracy: 0.8958\n",
      "Epoch 181: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4055 - accuracy: 0.8878 - val_loss: 1.9543 - val_accuracy: 0.8000\n",
      "Epoch 182/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4585 - accuracy: 0.8542\n",
      "Epoch 182: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4504 - accuracy: 0.8571 - val_loss: 1.9187 - val_accuracy: 0.8000\n",
      "Epoch 183/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.4559 - accuracy: 0.8438\n",
      "Epoch 183: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.4470 - accuracy: 0.8469 - val_loss: 1.9183 - val_accuracy: 0.8000\n",
      "Epoch 184/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3534 - accuracy: 0.8750\n",
      "Epoch 184: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.3474 - accuracy: 0.8776 - val_loss: 1.9157 - val_accuracy: 0.8000\n",
      "Epoch 185/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3334 - accuracy: 0.8750\n",
      "Epoch 185: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.3285 - accuracy: 0.8776 - val_loss: 1.9061 - val_accuracy: 0.7600\n",
      "Epoch 186/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2323 - accuracy: 0.9271\n",
      "Epoch 186: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2453 - accuracy: 0.9184 - val_loss: 1.8787 - val_accuracy: 0.8000\n",
      "Epoch 187/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3826 - accuracy: 0.8646\n",
      "Epoch 187: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.3943 - accuracy: 0.8571 - val_loss: 1.9104 - val_accuracy: 0.8000\n",
      "Epoch 188/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3094 - accuracy: 0.9062\n",
      "Epoch 188: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.3186 - accuracy: 0.8980 - val_loss: 1.8531 - val_accuracy: 0.7600\n",
      "Epoch 189/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3633 - accuracy: 0.8646\n",
      "Epoch 189: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.3557 - accuracy: 0.8673 - val_loss: 1.8802 - val_accuracy: 0.8000\n",
      "Epoch 190/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3287 - accuracy: 0.8438\n",
      "Epoch 190: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.3220 - accuracy: 0.8469 - val_loss: 1.8546 - val_accuracy: 0.8000\n",
      "Epoch 191/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2332 - accuracy: 0.9271\n",
      "Epoch 191: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.2356 - accuracy: 0.9286 - val_loss: 1.7995 - val_accuracy: 0.8400\n",
      "Epoch 192/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.3679 - accuracy: 0.8469\n",
      "Epoch 192: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.3679 - accuracy: 0.8469 - val_loss: 1.8179 - val_accuracy: 0.7600\n",
      "Epoch 193/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3278 - accuracy: 0.8958\n",
      "Epoch 193: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.3206 - accuracy: 0.8980 - val_loss: 1.8175 - val_accuracy: 0.8000\n",
      "Epoch 194/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3446 - accuracy: 0.8750\n",
      "Epoch 194: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.3721 - accuracy: 0.8571 - val_loss: 1.8021 - val_accuracy: 0.8000\n",
      "Epoch 195/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2854 - accuracy: 0.8646\n",
      "Epoch 195: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2793 - accuracy: 0.8673 - val_loss: 1.7696 - val_accuracy: 0.8400\n",
      "Epoch 196/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2605 - accuracy: 0.8646\n",
      "Epoch 196: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2549 - accuracy: 0.8673 - val_loss: 1.7395 - val_accuracy: 0.8400\n",
      "Epoch 197/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2588 - accuracy: 0.9062\n",
      "Epoch 197: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.2528 - accuracy: 0.9082 - val_loss: 1.7572 - val_accuracy: 0.8400\n",
      "Epoch 198/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1984 - accuracy: 0.9167\n",
      "Epoch 198: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.1944 - accuracy: 0.9184 - val_loss: 1.7216 - val_accuracy: 0.8400\n",
      "Epoch 199/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3287 - accuracy: 0.8333\n",
      "Epoch 199: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.3222 - accuracy: 0.8367 - val_loss: 1.7373 - val_accuracy: 0.8000\n",
      "Epoch 200/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2528 - accuracy: 0.9167\n",
      "Epoch 200: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2693 - accuracy: 0.9082 - val_loss: 1.7439 - val_accuracy: 0.8400\n",
      "Epoch 201/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3339 - accuracy: 0.8542\n",
      "Epoch 201: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.3569 - accuracy: 0.8469 - val_loss: 1.7512 - val_accuracy: 0.8400\n",
      "Epoch 202/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2648 - accuracy: 0.8854\n",
      "Epoch 202: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2606 - accuracy: 0.8878 - val_loss: 1.7420 - val_accuracy: 0.8000\n",
      "Epoch 203/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2333 - accuracy: 0.9062\n",
      "Epoch 203: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2410 - accuracy: 0.8980 - val_loss: 1.7691 - val_accuracy: 0.7600\n",
      "Epoch 204/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1290 - accuracy: 0.9375\n",
      "Epoch 204: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1270 - accuracy: 0.9388 - val_loss: 1.7355 - val_accuracy: 0.7600\n",
      "Epoch 205/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2348 - accuracy: 0.8958\n",
      "Epoch 205: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.2322 - accuracy: 0.8980 - val_loss: 1.7413 - val_accuracy: 0.8000\n",
      "Epoch 206/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2995 - accuracy: 0.8125\n",
      "Epoch 206: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2925 - accuracy: 0.8163 - val_loss: 1.7191 - val_accuracy: 0.8000\n",
      "Epoch 207/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.2476 - accuracy: 0.9184\n",
      "Epoch 207: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.2476 - accuracy: 0.9184 - val_loss: 1.7194 - val_accuracy: 0.7600\n",
      "Epoch 208/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2059 - accuracy: 0.9062\n",
      "Epoch 208: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2005 - accuracy: 0.9082 - val_loss: 1.6843 - val_accuracy: 0.8000\n",
      "Epoch 209/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1479 - accuracy: 0.9375\n",
      "Epoch 209: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1459 - accuracy: 0.9388 - val_loss: 1.7044 - val_accuracy: 0.7200\n",
      "Epoch 210/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2322 - accuracy: 0.9062\n",
      "Epoch 210: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2260 - accuracy: 0.9082 - val_loss: 1.6414 - val_accuracy: 0.7200\n",
      "Epoch 211/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2129 - accuracy: 0.9062\n",
      "Epoch 211: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.2083 - accuracy: 0.9082 - val_loss: 1.6203 - val_accuracy: 0.8000\n",
      "Epoch 212/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1424 - accuracy: 0.9271\n",
      "Epoch 212: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.1409 - accuracy: 0.9286 - val_loss: 1.6431 - val_accuracy: 0.7600\n",
      "Epoch 213/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1851 - accuracy: 0.9375\n",
      "Epoch 213: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.1809 - accuracy: 0.9388 - val_loss: 1.6188 - val_accuracy: 0.7600\n",
      "Epoch 214/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1704 - accuracy: 0.9271\n",
      "Epoch 214: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1676 - accuracy: 0.9286 - val_loss: 1.6154 - val_accuracy: 0.8000\n",
      "Epoch 215/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1518 - accuracy: 0.9062\n",
      "Epoch 215: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.1488 - accuracy: 0.9082 - val_loss: 1.6330 - val_accuracy: 0.7600\n",
      "Epoch 216/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.1664 - accuracy: 0.8776\n",
      "Epoch 216: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 1.1664 - accuracy: 0.8776 - val_loss: 1.6077 - val_accuracy: 0.8000\n",
      "Epoch 217/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1647 - accuracy: 0.9167\n",
      "Epoch 217: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1609 - accuracy: 0.9184 - val_loss: 1.6121 - val_accuracy: 0.7600\n",
      "Epoch 218/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1049 - accuracy: 0.9167\n",
      "Epoch 218: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1063 - accuracy: 0.9184 - val_loss: 1.5807 - val_accuracy: 0.8400\n",
      "Epoch 219/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.3024 - accuracy: 0.8229\n",
      "Epoch 219: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.3081 - accuracy: 0.8163 - val_loss: 1.6167 - val_accuracy: 0.8000\n",
      "Epoch 220/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2118 - accuracy: 0.8646\n",
      "Epoch 220: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.2048 - accuracy: 0.8673 - val_loss: 1.6246 - val_accuracy: 0.8000\n",
      "Epoch 221/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.2301 - accuracy: 0.8542\n",
      "Epoch 221: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2243 - accuracy: 0.8571 - val_loss: 1.5872 - val_accuracy: 0.8000\n",
      "Epoch 222/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.1006 - accuracy: 0.9388\n",
      "Epoch 222: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.1006 - accuracy: 0.9388 - val_loss: 1.5755 - val_accuracy: 0.8000\n",
      "Epoch 223/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1197 - accuracy: 0.8854\n",
      "Epoch 223: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.1144 - accuracy: 0.8878 - val_loss: 1.5720 - val_accuracy: 0.8000\n",
      "Epoch 224/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0887 - accuracy: 0.9375\n",
      "Epoch 224: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.0927 - accuracy: 0.9286 - val_loss: 1.5432 - val_accuracy: 0.8400\n",
      "Epoch 225/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1211 - accuracy: 0.9167\n",
      "Epoch 225: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1159 - accuracy: 0.9184 - val_loss: 1.5938 - val_accuracy: 0.8000\n",
      "Epoch 226/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1086 - accuracy: 0.8854\n",
      "Epoch 226: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1065 - accuracy: 0.8878 - val_loss: 1.5503 - val_accuracy: 0.8000\n",
      "Epoch 227/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0805 - accuracy: 0.9062\n",
      "Epoch 227: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.0784 - accuracy: 0.9082 - val_loss: 1.5809 - val_accuracy: 0.8000\n",
      "Epoch 228/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1585 - accuracy: 0.9167\n",
      "Epoch 228: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.1523 - accuracy: 0.9184 - val_loss: 1.5558 - val_accuracy: 0.7600\n",
      "Epoch 229/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0365 - accuracy: 0.9271\n",
      "Epoch 229: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.0331 - accuracy: 0.9286 - val_loss: 1.5200 - val_accuracy: 0.8000\n",
      "Epoch 230/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.0592 - accuracy: 0.9286\n",
      "Epoch 230: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 1.0592 - accuracy: 0.9286 - val_loss: 1.6020 - val_accuracy: 0.8000\n",
      "Epoch 231/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0780 - accuracy: 0.9271\n",
      "Epoch 231: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.0779 - accuracy: 0.9286 - val_loss: 1.6022 - val_accuracy: 0.7200\n",
      "Epoch 232/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9819 - accuracy: 0.9375\n",
      "Epoch 232: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9949 - accuracy: 0.9286 - val_loss: 1.5380 - val_accuracy: 0.8000\n",
      "Epoch 233/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0748 - accuracy: 0.8958\n",
      "Epoch 233: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.0726 - accuracy: 0.8980 - val_loss: 1.5371 - val_accuracy: 0.7600\n",
      "Epoch 234/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0925 - accuracy: 0.8750\n",
      "Epoch 234: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.0970 - accuracy: 0.8776 - val_loss: 1.4901 - val_accuracy: 0.8800\n",
      "Epoch 235/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0844 - accuracy: 0.9167\n",
      "Epoch 235: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 1.0826 - accuracy: 0.9184 - val_loss: 1.5299 - val_accuracy: 0.7600\n",
      "Epoch 236/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0387 - accuracy: 0.9375\n",
      "Epoch 236: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 1.0414 - accuracy: 0.9388 - val_loss: 1.4819 - val_accuracy: 0.8400\n",
      "Epoch 237/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1242 - accuracy: 0.8854\n",
      "Epoch 237: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 1.1183 - accuracy: 0.8878 - val_loss: 1.4809 - val_accuracy: 0.8000\n",
      "Epoch 238/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.1052 - accuracy: 0.9271\n",
      "Epoch 238: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 1.1012 - accuracy: 0.9286 - val_loss: 1.4784 - val_accuracy: 0.8400\n",
      "Epoch 239/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0080 - accuracy: 0.9271\n",
      "Epoch 239: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 1.0110 - accuracy: 0.9286 - val_loss: 1.4979 - val_accuracy: 0.8000\n",
      "Epoch 240/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9611 - accuracy: 0.9375\n",
      "Epoch 240: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.9581 - accuracy: 0.9388 - val_loss: 1.4931 - val_accuracy: 0.8000\n",
      "Epoch 241/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0177 - accuracy: 0.9375\n",
      "Epoch 241: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 1.0134 - accuracy: 0.9388 - val_loss: 1.4663 - val_accuracy: 0.8000\n",
      "Epoch 242/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9543 - accuracy: 0.9592\n",
      "Epoch 242: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9543 - accuracy: 0.9592 - val_loss: 1.5175 - val_accuracy: 0.7600\n",
      "Epoch 243/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9625 - accuracy: 0.9479\n",
      "Epoch 243: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9623 - accuracy: 0.9490 - val_loss: 1.4635 - val_accuracy: 0.8400\n",
      "Epoch 244/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9842 - accuracy: 0.9271\n",
      "Epoch 244: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.9803 - accuracy: 0.9286 - val_loss: 1.4693 - val_accuracy: 0.8400\n",
      "Epoch 245/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9726 - accuracy: 0.9479\n",
      "Epoch 245: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 1.0273 - accuracy: 0.9388 - val_loss: 1.4526 - val_accuracy: 0.8000\n",
      "Epoch 246/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9963 - accuracy: 0.9167\n",
      "Epoch 246: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.9938 - accuracy: 0.9184 - val_loss: 1.4695 - val_accuracy: 0.8000\n",
      "Epoch 247/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0102 - accuracy: 0.9167\n",
      "Epoch 247: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 1.0235 - accuracy: 0.9082 - val_loss: 1.5210 - val_accuracy: 0.6400\n",
      "Epoch 248/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9784 - accuracy: 0.9375\n",
      "Epoch 248: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9754 - accuracy: 0.9388 - val_loss: 1.4911 - val_accuracy: 0.7200\n",
      "Epoch 249/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9816 - accuracy: 0.9082\n",
      "Epoch 249: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.9816 - accuracy: 0.9082 - val_loss: 1.4961 - val_accuracy: 0.6800\n",
      "Epoch 250/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9375 - accuracy: 0.9583\n",
      "Epoch 250: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.9361 - accuracy: 0.9592 - val_loss: 1.4590 - val_accuracy: 0.7600\n",
      "Epoch 251/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0133 - accuracy: 0.9062\n",
      "Epoch 251: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 1.0204 - accuracy: 0.9082 - val_loss: 1.5235 - val_accuracy: 0.7600\n",
      "Epoch 252/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9657 - accuracy: 0.9062\n",
      "Epoch 252: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 0.9618 - accuracy: 0.9082 - val_loss: 1.4951 - val_accuracy: 0.8000\n",
      "Epoch 253/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0473 - accuracy: 0.8958\n",
      "Epoch 253: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 1.0421 - accuracy: 0.8980 - val_loss: 1.4594 - val_accuracy: 0.8400\n",
      "Epoch 254/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0434 - accuracy: 0.8958\n",
      "Epoch 254: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 1.0387 - accuracy: 0.8980 - val_loss: 1.4679 - val_accuracy: 0.8000\n",
      "Epoch 255/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9706 - accuracy: 0.9271\n",
      "Epoch 255: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.9663 - accuracy: 0.9286 - val_loss: 1.4493 - val_accuracy: 0.8000\n",
      "Epoch 256/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9249 - accuracy: 0.9167\n",
      "Epoch 256: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.9306 - accuracy: 0.9184 - val_loss: 1.4151 - val_accuracy: 0.8800\n",
      "Epoch 257/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9479 - accuracy: 0.9375\n",
      "Epoch 257: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.9554 - accuracy: 0.9286 - val_loss: 1.4241 - val_accuracy: 0.8000\n",
      "Epoch 258/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9335 - accuracy: 0.9062\n",
      "Epoch 258: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9403 - accuracy: 0.8980 - val_loss: 1.4638 - val_accuracy: 0.8000\n",
      "Epoch 259/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9230 - accuracy: 0.9271\n",
      "Epoch 259: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.9246 - accuracy: 0.9286 - val_loss: 1.4460 - val_accuracy: 0.8000\n",
      "Epoch 260/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9035 - accuracy: 0.9694\n",
      "Epoch 260: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.9035 - accuracy: 0.9694 - val_loss: 1.4583 - val_accuracy: 0.7600\n",
      "Epoch 261/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9485 - accuracy: 0.9167\n",
      "Epoch 261: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9502 - accuracy: 0.9184 - val_loss: 1.4400 - val_accuracy: 0.8000\n",
      "Epoch 262/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8891 - accuracy: 0.9479\n",
      "Epoch 262: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.8871 - accuracy: 0.9490 - val_loss: 1.4524 - val_accuracy: 0.6800\n",
      "Epoch 263/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9476 - accuracy: 0.9375\n",
      "Epoch 263: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.9434 - accuracy: 0.9388 - val_loss: 1.4397 - val_accuracy: 0.7200\n",
      "Epoch 264/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9307 - accuracy: 0.9388\n",
      "Epoch 264: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9307 - accuracy: 0.9388 - val_loss: 1.4362 - val_accuracy: 0.8400\n",
      "Epoch 265/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9737 - accuracy: 0.9062\n",
      "Epoch 265: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.9695 - accuracy: 0.9082 - val_loss: 1.4473 - val_accuracy: 0.8000\n",
      "Epoch 266/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9574 - accuracy: 0.9062\n",
      "Epoch 266: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9686 - accuracy: 0.8980 - val_loss: 1.4834 - val_accuracy: 0.6800\n",
      "Epoch 267/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9248 - accuracy: 0.9479\n",
      "Epoch 267: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9214 - accuracy: 0.9490 - val_loss: 1.4560 - val_accuracy: 0.6400\n",
      "Epoch 268/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8855 - accuracy: 0.9271\n",
      "Epoch 268: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.8823 - accuracy: 0.9286 - val_loss: 1.4098 - val_accuracy: 0.8000\n",
      "Epoch 269/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9415 - accuracy: 0.9062\n",
      "Epoch 269: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9373 - accuracy: 0.9082 - val_loss: 1.4086 - val_accuracy: 0.6800\n",
      "Epoch 270/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8763 - accuracy: 0.9583\n",
      "Epoch 270: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.8795 - accuracy: 0.9592 - val_loss: 1.3911 - val_accuracy: 0.7600\n",
      "Epoch 271/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8356 - accuracy: 0.9694\n",
      "Epoch 271: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.8356 - accuracy: 0.9694 - val_loss: 1.4142 - val_accuracy: 0.6800\n",
      "Epoch 272/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9301 - accuracy: 0.9375\n",
      "Epoch 272: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.9258 - accuracy: 0.9388 - val_loss: 1.3801 - val_accuracy: 0.8400\n",
      "Epoch 273/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8184 - accuracy: 0.9688\n",
      "Epoch 273: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.8161 - accuracy: 0.9694 - val_loss: 1.3936 - val_accuracy: 0.8000\n",
      "Epoch 274/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9514 - accuracy: 0.8958\n",
      "Epoch 274: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.9558 - accuracy: 0.8878 - val_loss: 1.4272 - val_accuracy: 0.6800\n",
      "Epoch 275/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8847 - accuracy: 0.9375\n",
      "Epoch 275: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.8828 - accuracy: 0.9388 - val_loss: 1.3777 - val_accuracy: 0.7600\n",
      "Epoch 276/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8195 - accuracy: 0.9583\n",
      "Epoch 276: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.8182 - accuracy: 0.9592 - val_loss: 1.3267 - val_accuracy: 0.8400\n",
      "Epoch 277/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8993 - accuracy: 0.9375\n",
      "Epoch 277: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.8955 - accuracy: 0.9388 - val_loss: 1.3092 - val_accuracy: 0.7600\n",
      "Epoch 278/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9046 - accuracy: 0.9271\n",
      "Epoch 278: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.9013 - accuracy: 0.9286 - val_loss: 1.2997 - val_accuracy: 0.8400\n",
      "Epoch 279/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9056 - accuracy: 0.9479\n",
      "Epoch 279: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9046 - accuracy: 0.9490 - val_loss: 1.3666 - val_accuracy: 0.7600\n",
      "Epoch 280/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.9490\n",
      "Epoch 280: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.8255 - accuracy: 0.9490 - val_loss: 1.2963 - val_accuracy: 0.8400\n",
      "Epoch 281/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9260 - accuracy: 0.9271\n",
      "Epoch 281: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.9363 - accuracy: 0.9184 - val_loss: 1.3319 - val_accuracy: 0.8400\n",
      "Epoch 282/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8903 - accuracy: 0.9184\n",
      "Epoch 282: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.8903 - accuracy: 0.9184 - val_loss: 1.3445 - val_accuracy: 0.7200\n",
      "Epoch 283/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.9388\n",
      "Epoch 283: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.8339 - accuracy: 0.9388 - val_loss: 1.3133 - val_accuracy: 0.7200\n",
      "Epoch 284/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8700 - accuracy: 0.9184\n",
      "Epoch 284: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.8700 - accuracy: 0.9184 - val_loss: 1.3393 - val_accuracy: 0.7200\n",
      "Epoch 285/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7866 - accuracy: 0.9583\n",
      "Epoch 285: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.7860 - accuracy: 0.9592 - val_loss: 1.3174 - val_accuracy: 0.6800\n",
      "Epoch 286/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7847 - accuracy: 0.9592\n",
      "Epoch 286: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.7847 - accuracy: 0.9592 - val_loss: 1.3126 - val_accuracy: 0.7200\n",
      "Epoch 287/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8673 - accuracy: 0.9286\n",
      "Epoch 287: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.8673 - accuracy: 0.9286 - val_loss: 1.3126 - val_accuracy: 0.7200\n",
      "Epoch 288/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.9140 - accuracy: 0.8958\n",
      "Epoch 288: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9096 - accuracy: 0.8980 - val_loss: 1.3314 - val_accuracy: 0.7600\n",
      "Epoch 289/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7825 - accuracy: 0.9688\n",
      "Epoch 289: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.7803 - accuracy: 0.9694 - val_loss: 1.3047 - val_accuracy: 0.7200\n",
      "Epoch 290/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8259 - accuracy: 0.9583\n",
      "Epoch 290: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.8227 - accuracy: 0.9592 - val_loss: 1.3011 - val_accuracy: 0.7600\n",
      "Epoch 291/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7772 - accuracy: 0.9592\n",
      "Epoch 291: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.7772 - accuracy: 0.9592 - val_loss: 1.3160 - val_accuracy: 0.7200\n",
      "Epoch 292/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7810 - accuracy: 0.9583\n",
      "Epoch 292: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.7793 - accuracy: 0.9592 - val_loss: 1.3443 - val_accuracy: 0.7200\n",
      "Epoch 293/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8133 - accuracy: 0.9583\n",
      "Epoch 293: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.8102 - accuracy: 0.9592 - val_loss: 1.2653 - val_accuracy: 0.8000\n",
      "Epoch 294/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8012 - accuracy: 0.9583\n",
      "Epoch 294: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.7993 - accuracy: 0.9592 - val_loss: 1.2644 - val_accuracy: 0.7600\n",
      "Epoch 295/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7696 - accuracy: 0.9490\n",
      "Epoch 295: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.7696 - accuracy: 0.9490 - val_loss: 1.2722 - val_accuracy: 0.8000\n",
      "Epoch 296/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7522 - accuracy: 0.9796\n",
      "Epoch 296: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.7522 - accuracy: 0.9796 - val_loss: 1.3147 - val_accuracy: 0.7200\n",
      "Epoch 297/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7805 - accuracy: 0.9167\n",
      "Epoch 297: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.7777 - accuracy: 0.9184 - val_loss: 1.2565 - val_accuracy: 0.7200\n",
      "Epoch 298/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7547 - accuracy: 0.9479\n",
      "Epoch 298: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.7555 - accuracy: 0.9490 - val_loss: 1.2993 - val_accuracy: 0.7200\n",
      "Epoch 299/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7811 - accuracy: 0.9479\n",
      "Epoch 299: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.7799 - accuracy: 0.9490 - val_loss: 1.3105 - val_accuracy: 0.6800\n",
      "Epoch 300/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7265 - accuracy: 0.9688\n",
      "Epoch 300: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7253 - accuracy: 0.9694 - val_loss: 1.2484 - val_accuracy: 0.7600\n",
      "Epoch 301/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7403 - accuracy: 0.9688\n",
      "Epoch 301: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.7381 - accuracy: 0.9694 - val_loss: 1.2327 - val_accuracy: 0.7600\n",
      "Epoch 302/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8395 - accuracy: 0.9286\n",
      "Epoch 302: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.8395 - accuracy: 0.9286 - val_loss: 1.2426 - val_accuracy: 0.7200\n",
      "Epoch 303/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8375 - accuracy: 0.9167\n",
      "Epoch 303: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 0.8352 - accuracy: 0.9184 - val_loss: 1.3238 - val_accuracy: 0.7200\n",
      "Epoch 304/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7964 - accuracy: 0.9375\n",
      "Epoch 304: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.8259 - accuracy: 0.9286 - val_loss: 1.2674 - val_accuracy: 0.7200\n",
      "Epoch 305/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8381 - accuracy: 0.9375\n",
      "Epoch 305: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.8336 - accuracy: 0.9388 - val_loss: 1.2255 - val_accuracy: 0.7200\n",
      "Epoch 306/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7570 - accuracy: 0.9583\n",
      "Epoch 306: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.7556 - accuracy: 0.9592 - val_loss: 1.2490 - val_accuracy: 0.6800\n",
      "Epoch 307/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8318 - accuracy: 0.9184\n",
      "Epoch 307: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.8318 - accuracy: 0.9184 - val_loss: 1.2307 - val_accuracy: 0.7200\n",
      "Epoch 308/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.9898\n",
      "Epoch 308: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.6820 - accuracy: 0.9898 - val_loss: 1.2168 - val_accuracy: 0.7600\n",
      "Epoch 309/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7685 - accuracy: 0.9271\n",
      "Epoch 309: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.7828 - accuracy: 0.9184 - val_loss: 1.2246 - val_accuracy: 0.7200\n",
      "Epoch 310/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7567 - accuracy: 0.9479\n",
      "Epoch 310: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.7570 - accuracy: 0.9490 - val_loss: 1.2937 - val_accuracy: 0.6800\n",
      "Epoch 311/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6962 - accuracy: 0.9792\n",
      "Epoch 311: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.6951 - accuracy: 0.9796 - val_loss: 1.2511 - val_accuracy: 0.6800\n",
      "Epoch 312/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7742 - accuracy: 0.9583\n",
      "Epoch 312: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7710 - accuracy: 0.9592 - val_loss: 1.2235 - val_accuracy: 0.7600\n",
      "Epoch 313/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.9592\n",
      "Epoch 313: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.7162 - accuracy: 0.9592 - val_loss: 1.2214 - val_accuracy: 0.7600\n",
      "Epoch 314/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7700 - accuracy: 0.9583\n",
      "Epoch 314: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7683 - accuracy: 0.9592 - val_loss: 1.2183 - val_accuracy: 0.8000\n",
      "Epoch 315/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8144 - accuracy: 0.9271\n",
      "Epoch 315: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.8108 - accuracy: 0.9286 - val_loss: 1.2560 - val_accuracy: 0.7600\n",
      "Epoch 316/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7619 - accuracy: 0.9479\n",
      "Epoch 316: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.7650 - accuracy: 0.9490 - val_loss: 1.2201 - val_accuracy: 0.7600\n",
      "Epoch 317/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7129 - accuracy: 0.9592\n",
      "Epoch 317: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.7129 - accuracy: 0.9592 - val_loss: 1.2909 - val_accuracy: 0.7200\n",
      "Epoch 318/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7587 - accuracy: 0.9375\n",
      "Epoch 318: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7556 - accuracy: 0.9388 - val_loss: 1.2320 - val_accuracy: 0.6800\n",
      "Epoch 319/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7517 - accuracy: 0.9479\n",
      "Epoch 319: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7498 - accuracy: 0.9490 - val_loss: 1.2230 - val_accuracy: 0.6800\n",
      "Epoch 320/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7577 - accuracy: 0.9592\n",
      "Epoch 320: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.7577 - accuracy: 0.9592 - val_loss: 1.2485 - val_accuracy: 0.7200\n",
      "Epoch 321/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7464 - accuracy: 0.9479\n",
      "Epoch 321: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7430 - accuracy: 0.9490 - val_loss: 1.2200 - val_accuracy: 0.6800\n",
      "Epoch 322/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6742 - accuracy: 0.9688\n",
      "Epoch 322: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6765 - accuracy: 0.9694 - val_loss: 1.1768 - val_accuracy: 0.8000\n",
      "Epoch 323/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7475 - accuracy: 0.9271\n",
      "Epoch 323: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.7441 - accuracy: 0.9286 - val_loss: 1.2226 - val_accuracy: 0.6800\n",
      "Epoch 324/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7157 - accuracy: 0.9583\n",
      "Epoch 324: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.7195 - accuracy: 0.9592 - val_loss: 1.2384 - val_accuracy: 0.6800\n",
      "Epoch 325/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7895 - accuracy: 0.9271\n",
      "Epoch 325: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7851 - accuracy: 0.9286 - val_loss: 1.2132 - val_accuracy: 0.7600\n",
      "Epoch 326/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7996 - accuracy: 0.9271\n",
      "Epoch 326: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.7966 - accuracy: 0.9286 - val_loss: 1.1855 - val_accuracy: 0.7600\n",
      "Epoch 327/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6844 - accuracy: 0.9583\n",
      "Epoch 327: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6829 - accuracy: 0.9592 - val_loss: 1.1772 - val_accuracy: 0.8400\n",
      "Epoch 328/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6644 - accuracy: 0.9688\n",
      "Epoch 328: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6625 - accuracy: 0.9694 - val_loss: 1.1430 - val_accuracy: 0.8400\n",
      "Epoch 329/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6722 - accuracy: 0.9792\n",
      "Epoch 329: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6701 - accuracy: 0.9796 - val_loss: 1.1816 - val_accuracy: 0.7600\n",
      "Epoch 330/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7314 - accuracy: 0.9694\n",
      "Epoch 330: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.7314 - accuracy: 0.9694 - val_loss: 1.1525 - val_accuracy: 0.7600\n",
      "Epoch 331/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6617 - accuracy: 0.9688\n",
      "Epoch 331: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6638 - accuracy: 0.9694 - val_loss: 1.1066 - val_accuracy: 0.8800\n",
      "Epoch 332/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7082 - accuracy: 0.9479\n",
      "Epoch 332: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.7262 - accuracy: 0.9388 - val_loss: 1.1100 - val_accuracy: 0.8400\n",
      "Epoch 333/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6571 - accuracy: 0.9688\n",
      "Epoch 333: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.6557 - accuracy: 0.9694 - val_loss: 1.1368 - val_accuracy: 0.7600\n",
      "Epoch 334/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7166 - accuracy: 0.9375\n",
      "Epoch 334: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.7150 - accuracy: 0.9388 - val_loss: 1.1728 - val_accuracy: 0.7200\n",
      "Epoch 335/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7318 - accuracy: 0.9271\n",
      "Epoch 335: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.7282 - accuracy: 0.9286 - val_loss: 1.1501 - val_accuracy: 0.8000\n",
      "Epoch 336/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6486 - accuracy: 0.9490\n",
      "Epoch 336: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.6486 - accuracy: 0.9490 - val_loss: 1.1783 - val_accuracy: 0.7200\n",
      "Epoch 337/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6329 - accuracy: 0.9688\n",
      "Epoch 337: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.6313 - accuracy: 0.9694 - val_loss: 1.1580 - val_accuracy: 0.7200\n",
      "Epoch 338/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6962 - accuracy: 0.9583\n",
      "Epoch 338: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.7018 - accuracy: 0.9592 - val_loss: 1.1623 - val_accuracy: 0.7600\n",
      "Epoch 339/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.9694\n",
      "Epoch 339: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.6992 - accuracy: 0.9694 - val_loss: 1.1496 - val_accuracy: 0.7200\n",
      "Epoch 340/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6895 - accuracy: 0.9479\n",
      "Epoch 340: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.6865 - accuracy: 0.9490 - val_loss: 1.1215 - val_accuracy: 0.7200\n",
      "Epoch 341/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6312 - accuracy: 0.9792\n",
      "Epoch 341: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.6311 - accuracy: 0.9796 - val_loss: 1.1280 - val_accuracy: 0.6800\n",
      "Epoch 342/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7177 - accuracy: 0.9062\n",
      "Epoch 342: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7141 - accuracy: 0.9082 - val_loss: 1.1193 - val_accuracy: 0.8000\n",
      "Epoch 343/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7147 - accuracy: 0.9375\n",
      "Epoch 343: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.7122 - accuracy: 0.9388 - val_loss: 1.1494 - val_accuracy: 0.6800\n",
      "Epoch 344/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6320 - accuracy: 0.9694\n",
      "Epoch 344: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.6320 - accuracy: 0.9694 - val_loss: 1.1588 - val_accuracy: 0.6800\n",
      "Epoch 345/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6103 - accuracy: 0.9792\n",
      "Epoch 345: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.6123 - accuracy: 0.9796 - val_loss: 1.1395 - val_accuracy: 0.7200\n",
      "Epoch 346/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6437 - accuracy: 0.9375\n",
      "Epoch 346: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6414 - accuracy: 0.9388 - val_loss: 1.1205 - val_accuracy: 0.8000\n",
      "Epoch 347/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6216 - accuracy: 0.9688\n",
      "Epoch 347: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.6208 - accuracy: 0.9694 - val_loss: 1.1201 - val_accuracy: 0.8000\n",
      "Epoch 348/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5941 - accuracy: 0.9896\n",
      "Epoch 348: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5942 - accuracy: 0.9898 - val_loss: 1.1384 - val_accuracy: 0.8000\n",
      "Epoch 349/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6774 - accuracy: 0.9479\n",
      "Epoch 349: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6770 - accuracy: 0.9490 - val_loss: 1.1702 - val_accuracy: 0.6800\n",
      "Epoch 350/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6414 - accuracy: 0.9792\n",
      "Epoch 350: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.6389 - accuracy: 0.9796 - val_loss: 1.1354 - val_accuracy: 0.8000\n",
      "Epoch 351/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6817 - accuracy: 0.9271\n",
      "Epoch 351: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6797 - accuracy: 0.9286 - val_loss: 1.1351 - val_accuracy: 0.8400\n",
      "Epoch 352/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6379 - accuracy: 0.9792\n",
      "Epoch 352: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6371 - accuracy: 0.9796 - val_loss: 1.1265 - val_accuracy: 0.8000\n",
      "Epoch 353/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6706 - accuracy: 0.9271\n",
      "Epoch 353: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.6720 - accuracy: 0.9286 - val_loss: 1.1326 - val_accuracy: 0.7600\n",
      "Epoch 354/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6558 - accuracy: 0.9479\n",
      "Epoch 354: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6533 - accuracy: 0.9490 - val_loss: 1.1307 - val_accuracy: 0.7600\n",
      "Epoch 355/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7040 - accuracy: 0.9184\n",
      "Epoch 355: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.7040 - accuracy: 0.9184 - val_loss: 1.1780 - val_accuracy: 0.6800\n",
      "Epoch 356/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5652 - accuracy: 0.9792\n",
      "Epoch 356: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.5648 - accuracy: 0.9796 - val_loss: 1.1248 - val_accuracy: 0.7200\n",
      "Epoch 357/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7008 - accuracy: 0.9271\n",
      "Epoch 357: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.6977 - accuracy: 0.9286 - val_loss: 1.1567 - val_accuracy: 0.7200\n",
      "Epoch 358/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5626 - accuracy: 0.9792\n",
      "Epoch 358: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.5635 - accuracy: 0.9796 - val_loss: 1.1705 - val_accuracy: 0.7200\n",
      "Epoch 359/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6257 - accuracy: 0.9583\n",
      "Epoch 359: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.6233 - accuracy: 0.9592 - val_loss: 1.1103 - val_accuracy: 0.8400\n",
      "Epoch 360/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5573 - accuracy: 0.9896\n",
      "Epoch 360: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5626 - accuracy: 0.9898 - val_loss: 1.1053 - val_accuracy: 0.8400\n",
      "Epoch 361/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6401 - accuracy: 0.9583\n",
      "Epoch 361: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.6377 - accuracy: 0.9592 - val_loss: 1.0972 - val_accuracy: 0.8400\n",
      "Epoch 362/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6393 - accuracy: 0.9583\n",
      "Epoch 362: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6373 - accuracy: 0.9592 - val_loss: 1.0730 - val_accuracy: 0.8400\n",
      "Epoch 363/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6081 - accuracy: 0.9688\n",
      "Epoch 363: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6058 - accuracy: 0.9694 - val_loss: 1.1113 - val_accuracy: 0.7200\n",
      "Epoch 364/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.9796\n",
      "Epoch 364: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5701 - accuracy: 0.9796 - val_loss: 1.1363 - val_accuracy: 0.7200\n",
      "Epoch 365/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.9490\n",
      "Epoch 365: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.6433 - accuracy: 0.9490 - val_loss: 1.1734 - val_accuracy: 0.7200\n",
      "Epoch 366/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6078 - accuracy: 0.9479\n",
      "Epoch 366: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.6054 - accuracy: 0.9490 - val_loss: 1.1308 - val_accuracy: 0.8000\n",
      "Epoch 367/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6718 - accuracy: 0.9286\n",
      "Epoch 367: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.6718 - accuracy: 0.9286 - val_loss: 1.1218 - val_accuracy: 0.7200\n",
      "Epoch 368/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.9694\n",
      "Epoch 368: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5866 - accuracy: 0.9694 - val_loss: 1.1561 - val_accuracy: 0.7200\n",
      "Epoch 369/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.9796\n",
      "Epoch 369: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5777 - accuracy: 0.9796 - val_loss: 1.1079 - val_accuracy: 0.7200\n",
      "Epoch 370/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5643 - accuracy: 0.9688\n",
      "Epoch 370: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5912 - accuracy: 0.9592 - val_loss: 1.0517 - val_accuracy: 0.8400\n",
      "Epoch 371/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.7678 - accuracy: 0.9479\n",
      "Epoch 371: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.7620 - accuracy: 0.9490 - val_loss: 1.0982 - val_accuracy: 0.7600\n",
      "Epoch 372/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5448 - accuracy: 0.9792\n",
      "Epoch 372: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.5435 - accuracy: 0.9796 - val_loss: 1.1044 - val_accuracy: 0.7600\n",
      "Epoch 373/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5507 - accuracy: 0.9792\n",
      "Epoch 373: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.5493 - accuracy: 0.9796 - val_loss: 1.0886 - val_accuracy: 0.8000\n",
      "Epoch 374/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5549 - accuracy: 0.9896\n",
      "Epoch 374: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.5551 - accuracy: 0.9898 - val_loss: 1.1015 - val_accuracy: 0.7200\n",
      "Epoch 375/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.9388\n",
      "Epoch 375: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.5909 - accuracy: 0.9388 - val_loss: 1.1179 - val_accuracy: 0.7200\n",
      "Epoch 376/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5081 - accuracy: 0.9896\n",
      "Epoch 376: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5206 - accuracy: 0.9898 - val_loss: 1.1063 - val_accuracy: 0.7200\n",
      "Epoch 377/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6022 - accuracy: 0.9375\n",
      "Epoch 377: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6000 - accuracy: 0.9388 - val_loss: 1.0872 - val_accuracy: 0.7600\n",
      "Epoch 378/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6181 - accuracy: 0.9479\n",
      "Epoch 378: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6151 - accuracy: 0.9490 - val_loss: 1.1171 - val_accuracy: 0.7200\n",
      "Epoch 379/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.9694\n",
      "Epoch 379: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.5715 - accuracy: 0.9694 - val_loss: 1.1087 - val_accuracy: 0.7600\n",
      "Epoch 380/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 1.0000\n",
      "Epoch 380: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4989 - accuracy: 1.0000 - val_loss: 1.0899 - val_accuracy: 0.7200\n",
      "Epoch 381/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5852 - accuracy: 0.9479\n",
      "Epoch 381: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.5830 - accuracy: 0.9490 - val_loss: 1.0874 - val_accuracy: 0.7200\n",
      "Epoch 382/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5884 - accuracy: 0.9688\n",
      "Epoch 382: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5860 - accuracy: 0.9694 - val_loss: 1.1075 - val_accuracy: 0.6800\n",
      "Epoch 383/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5343 - accuracy: 0.9792\n",
      "Epoch 383: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5431 - accuracy: 0.9694 - val_loss: 1.1514 - val_accuracy: 0.6400\n",
      "Epoch 384/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6323 - accuracy: 0.9167\n",
      "Epoch 384: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6289 - accuracy: 0.9184 - val_loss: 1.1256 - val_accuracy: 0.7200\n",
      "Epoch 385/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5464 - accuracy: 0.9688\n",
      "Epoch 385: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5446 - accuracy: 0.9694 - val_loss: 1.0712 - val_accuracy: 0.8000\n",
      "Epoch 386/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5901 - accuracy: 0.9688\n",
      "Epoch 386: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5885 - accuracy: 0.9694 - val_loss: 1.1083 - val_accuracy: 0.7200\n",
      "Epoch 387/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5589 - accuracy: 0.9792\n",
      "Epoch 387: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5582 - accuracy: 0.9796 - val_loss: 1.0812 - val_accuracy: 0.8400\n",
      "Epoch 388/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5600 - accuracy: 0.9583\n",
      "Epoch 388: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5583 - accuracy: 0.9592 - val_loss: 1.0610 - val_accuracy: 0.8000\n",
      "Epoch 389/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5278 - accuracy: 0.9792\n",
      "Epoch 389: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5358 - accuracy: 0.9694 - val_loss: 1.1070 - val_accuracy: 0.7200\n",
      "Epoch 390/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5769 - accuracy: 0.9271\n",
      "Epoch 390: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5744 - accuracy: 0.9286 - val_loss: 1.0720 - val_accuracy: 0.7600\n",
      "Epoch 391/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6071 - accuracy: 0.9167\n",
      "Epoch 391: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.6038 - accuracy: 0.9184 - val_loss: 1.0647 - val_accuracy: 0.7600\n",
      "Epoch 392/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5274 - accuracy: 0.9896\n",
      "Epoch 392: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5259 - accuracy: 0.9898 - val_loss: 1.0726 - val_accuracy: 0.7200\n",
      "Epoch 393/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5237 - accuracy: 0.9792\n",
      "Epoch 393: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5220 - accuracy: 0.9796 - val_loss: 1.0662 - val_accuracy: 0.7200\n",
      "Epoch 394/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5566 - accuracy: 0.9479\n",
      "Epoch 394: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5555 - accuracy: 0.9490 - val_loss: 1.1301 - val_accuracy: 0.7200\n",
      "Epoch 395/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5139 - accuracy: 0.9792\n",
      "Epoch 395: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.5125 - accuracy: 0.9796 - val_loss: 1.1046 - val_accuracy: 0.7200\n",
      "Epoch 396/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5146 - accuracy: 0.9688\n",
      "Epoch 396: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.5237 - accuracy: 0.9592 - val_loss: 1.0374 - val_accuracy: 0.8400\n",
      "Epoch 397/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5642 - accuracy: 0.9375\n",
      "Epoch 397: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.5616 - accuracy: 0.9388 - val_loss: 1.0682 - val_accuracy: 0.8000\n",
      "Epoch 398/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5388 - accuracy: 0.9592\n",
      "Epoch 398: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5388 - accuracy: 0.9592 - val_loss: 1.0646 - val_accuracy: 0.7200\n",
      "Epoch 399/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5456 - accuracy: 0.9688\n",
      "Epoch 399: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.5482 - accuracy: 0.9694 - val_loss: 1.1136 - val_accuracy: 0.6800\n",
      "Epoch 400/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4967 - accuracy: 0.9896\n",
      "Epoch 400: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4953 - accuracy: 0.9898 - val_loss: 1.0596 - val_accuracy: 0.7600\n",
      "Epoch 401/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.9796\n",
      "Epoch 401: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5250 - accuracy: 0.9796 - val_loss: 1.0393 - val_accuracy: 0.8400\n",
      "Epoch 402/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5200 - accuracy: 0.9792\n",
      "Epoch 402: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5181 - accuracy: 0.9796 - val_loss: 1.0322 - val_accuracy: 0.8400\n",
      "Epoch 403/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5014 - accuracy: 0.9688\n",
      "Epoch 403: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.4999 - accuracy: 0.9694 - val_loss: 1.0564 - val_accuracy: 0.7200\n",
      "Epoch 404/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9490\n",
      "Epoch 404: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5125 - accuracy: 0.9490 - val_loss: 1.0272 - val_accuracy: 0.7600\n",
      "Epoch 405/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5936 - accuracy: 0.9479\n",
      "Epoch 405: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5910 - accuracy: 0.9490 - val_loss: 1.0757 - val_accuracy: 0.7600\n",
      "Epoch 406/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5085 - accuracy: 0.9688\n",
      "Epoch 406: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.5121 - accuracy: 0.9694 - val_loss: 1.1337 - val_accuracy: 0.7600\n",
      "Epoch 407/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5331 - accuracy: 0.9375\n",
      "Epoch 407: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.5308 - accuracy: 0.9388 - val_loss: 1.0912 - val_accuracy: 0.8000\n",
      "Epoch 408/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4966 - accuracy: 0.9792\n",
      "Epoch 408: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4974 - accuracy: 0.9796 - val_loss: 1.0389 - val_accuracy: 0.8400\n",
      "Epoch 409/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5160 - accuracy: 0.9792\n",
      "Epoch 409: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5150 - accuracy: 0.9796 - val_loss: 1.0700 - val_accuracy: 0.6800\n",
      "Epoch 410/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5074 - accuracy: 0.9688\n",
      "Epoch 410: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5072 - accuracy: 0.9694 - val_loss: 1.0628 - val_accuracy: 0.7600\n",
      "Epoch 411/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5472 - accuracy: 0.9271\n",
      "Epoch 411: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5446 - accuracy: 0.9286 - val_loss: 1.0376 - val_accuracy: 0.8000\n",
      "Epoch 412/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5067 - accuracy: 0.9792\n",
      "Epoch 412: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5068 - accuracy: 0.9796 - val_loss: 1.0828 - val_accuracy: 0.7200\n",
      "Epoch 413/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5010 - accuracy: 0.9688\n",
      "Epoch 413: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4992 - accuracy: 0.9694 - val_loss: 1.0540 - val_accuracy: 0.7600\n",
      "Epoch 414/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5548 - accuracy: 0.9479\n",
      "Epoch 414: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5619 - accuracy: 0.9388 - val_loss: 1.1261 - val_accuracy: 0.6800\n",
      "Epoch 415/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5387 - accuracy: 0.9375\n",
      "Epoch 415: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.5361 - accuracy: 0.9388 - val_loss: 1.0594 - val_accuracy: 0.8000\n",
      "Epoch 416/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5471 - accuracy: 0.9796\n",
      "Epoch 416: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5471 - accuracy: 0.9796 - val_loss: 1.0390 - val_accuracy: 0.8000\n",
      "Epoch 417/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9796\n",
      "Epoch 417: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5048 - accuracy: 0.9796 - val_loss: 1.0045 - val_accuracy: 0.8400\n",
      "Epoch 418/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4495 - accuracy: 1.0000\n",
      "Epoch 418: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4537 - accuracy: 1.0000 - val_loss: 1.0097 - val_accuracy: 0.8000\n",
      "Epoch 419/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4619 - accuracy: 0.9896\n",
      "Epoch 419: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4608 - accuracy: 0.9898 - val_loss: 1.0154 - val_accuracy: 0.8000\n",
      "Epoch 420/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4527 - accuracy: 0.9896\n",
      "Epoch 420: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4566 - accuracy: 0.9898 - val_loss: 0.9887 - val_accuracy: 0.8400\n",
      "Epoch 421/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5718 - accuracy: 0.9479\n",
      "Epoch 421: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.5700 - accuracy: 0.9490 - val_loss: 1.0175 - val_accuracy: 0.7200\n",
      "Epoch 422/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4963 - accuracy: 0.9583\n",
      "Epoch 422: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4993 - accuracy: 0.9592 - val_loss: 0.9996 - val_accuracy: 0.8400\n",
      "Epoch 423/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4866 - accuracy: 0.9688\n",
      "Epoch 423: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4896 - accuracy: 0.9694 - val_loss: 1.0144 - val_accuracy: 0.6800\n",
      "Epoch 424/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5567 - accuracy: 0.9375\n",
      "Epoch 424: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5860 - accuracy: 0.9286 - val_loss: 1.0210 - val_accuracy: 0.8400\n",
      "Epoch 425/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5271 - accuracy: 0.9583\n",
      "Epoch 425: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.5246 - accuracy: 0.9592 - val_loss: 1.0023 - val_accuracy: 0.8000\n",
      "Epoch 426/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9796\n",
      "Epoch 426: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.5048 - accuracy: 0.9796 - val_loss: 1.0100 - val_accuracy: 0.8000\n",
      "Epoch 427/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4717 - accuracy: 0.9688\n",
      "Epoch 427: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4719 - accuracy: 0.9694 - val_loss: 0.9892 - val_accuracy: 0.8000\n",
      "Epoch 428/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4455 - accuracy: 0.9896\n",
      "Epoch 428: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4449 - accuracy: 0.9898 - val_loss: 0.9790 - val_accuracy: 0.8000\n",
      "Epoch 429/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.9898\n",
      "Epoch 429: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4522 - accuracy: 0.9898 - val_loss: 0.9612 - val_accuracy: 0.8000\n",
      "Epoch 430/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4931 - accuracy: 0.9583\n",
      "Epoch 430: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4912 - accuracy: 0.9592 - val_loss: 0.9825 - val_accuracy: 0.7600\n",
      "Epoch 431/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4576 - accuracy: 0.9792\n",
      "Epoch 431: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4568 - accuracy: 0.9796 - val_loss: 0.9678 - val_accuracy: 0.8400\n",
      "Epoch 432/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.9592\n",
      "Epoch 432: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4853 - accuracy: 0.9592 - val_loss: 0.9495 - val_accuracy: 0.8000\n",
      "Epoch 433/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4341 - accuracy: 0.9896\n",
      "Epoch 433: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4332 - accuracy: 0.9898 - val_loss: 0.9383 - val_accuracy: 0.8000\n",
      "Epoch 434/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4497 - accuracy: 0.9792\n",
      "Epoch 434: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4586 - accuracy: 0.9694 - val_loss: 0.9553 - val_accuracy: 0.8000\n",
      "Epoch 435/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4876 - accuracy: 0.9583\n",
      "Epoch 435: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4863 - accuracy: 0.9592 - val_loss: 0.9492 - val_accuracy: 0.8000\n",
      "Epoch 436/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5491 - accuracy: 0.9583\n",
      "Epoch 436: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.5521 - accuracy: 0.9592 - val_loss: 1.0080 - val_accuracy: 0.7200\n",
      "Epoch 437/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4714 - accuracy: 0.9688\n",
      "Epoch 437: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4700 - accuracy: 0.9694 - val_loss: 1.0110 - val_accuracy: 0.7600\n",
      "Epoch 438/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4673 - accuracy: 0.9792\n",
      "Epoch 438: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4658 - accuracy: 0.9796 - val_loss: 1.0033 - val_accuracy: 0.7600\n",
      "Epoch 439/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4575 - accuracy: 0.9688\n",
      "Epoch 439: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4560 - accuracy: 0.9694 - val_loss: 0.9586 - val_accuracy: 0.7200\n",
      "Epoch 440/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4632 - accuracy: 0.9583\n",
      "Epoch 440: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4623 - accuracy: 0.9592 - val_loss: 0.9351 - val_accuracy: 0.7600\n",
      "Epoch 441/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4685 - accuracy: 0.9688\n",
      "Epoch 441: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4676 - accuracy: 0.9694 - val_loss: 0.9540 - val_accuracy: 0.7200\n",
      "Epoch 442/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4182 - accuracy: 0.9896\n",
      "Epoch 442: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4174 - accuracy: 0.9898 - val_loss: 0.9963 - val_accuracy: 0.6800\n",
      "Epoch 443/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4297 - accuracy: 1.0000\n",
      "Epoch 443: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4290 - accuracy: 1.0000 - val_loss: 1.0219 - val_accuracy: 0.6400\n",
      "Epoch 444/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4250 - accuracy: 0.9896\n",
      "Epoch 444: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4253 - accuracy: 0.9898 - val_loss: 0.9794 - val_accuracy: 0.6800\n",
      "Epoch 445/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.9694\n",
      "Epoch 445: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4809 - accuracy: 0.9694 - val_loss: 0.9812 - val_accuracy: 0.7200\n",
      "Epoch 446/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4468 - accuracy: 0.9796\n",
      "Epoch 446: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.4468 - accuracy: 0.9796 - val_loss: 0.9739 - val_accuracy: 0.7200\n",
      "Epoch 447/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4448 - accuracy: 1.0000\n",
      "Epoch 447: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4433 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.7200\n",
      "Epoch 448/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4381 - accuracy: 0.9896\n",
      "Epoch 448: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4436 - accuracy: 0.9898 - val_loss: 1.0078 - val_accuracy: 0.6800\n",
      "Epoch 449/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.9592\n",
      "Epoch 449: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5653 - accuracy: 0.9592 - val_loss: 0.9956 - val_accuracy: 0.6800\n",
      "Epoch 450/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4231 - accuracy: 0.9896\n",
      "Epoch 450: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4299 - accuracy: 0.9898 - val_loss: 0.9393 - val_accuracy: 0.7600\n",
      "Epoch 451/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4296 - accuracy: 0.9688\n",
      "Epoch 451: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4291 - accuracy: 0.9694 - val_loss: 0.9567 - val_accuracy: 0.6800\n",
      "Epoch 452/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4274 - accuracy: 0.9792\n",
      "Epoch 452: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4528 - accuracy: 0.9694 - val_loss: 0.9743 - val_accuracy: 0.7600\n",
      "Epoch 453/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.9388\n",
      "Epoch 453: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4961 - accuracy: 0.9388 - val_loss: 0.9679 - val_accuracy: 0.7200\n",
      "Epoch 454/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4824 - accuracy: 0.9271\n",
      "Epoch 454: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4807 - accuracy: 0.9286 - val_loss: 0.9589 - val_accuracy: 0.7600\n",
      "Epoch 455/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4681 - accuracy: 0.9792\n",
      "Epoch 455: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4666 - accuracy: 0.9796 - val_loss: 0.9342 - val_accuracy: 0.8000\n",
      "Epoch 456/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4186 - accuracy: 0.9896\n",
      "Epoch 456: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4178 - accuracy: 0.9898 - val_loss: 0.9024 - val_accuracy: 0.8400\n",
      "Epoch 457/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4221 - accuracy: 0.9896\n",
      "Epoch 457: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4208 - accuracy: 0.9898 - val_loss: 0.8952 - val_accuracy: 0.8800\n",
      "Epoch 458/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4576 - accuracy: 0.9792\n",
      "Epoch 458: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4562 - accuracy: 0.9796 - val_loss: 0.8840 - val_accuracy: 0.8400\n",
      "Epoch 459/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.9490\n",
      "Epoch 459: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4664 - accuracy: 0.9490 - val_loss: 0.8797 - val_accuracy: 0.8400\n",
      "Epoch 460/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4480 - accuracy: 0.9688\n",
      "Epoch 460: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4465 - accuracy: 0.9694 - val_loss: 0.8662 - val_accuracy: 0.8800\n",
      "Epoch 461/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4770 - accuracy: 0.9792\n",
      "Epoch 461: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4747 - accuracy: 0.9796 - val_loss: 0.9217 - val_accuracy: 0.7200\n",
      "Epoch 462/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4379 - accuracy: 0.9688\n",
      "Epoch 462: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4362 - accuracy: 0.9694 - val_loss: 0.9179 - val_accuracy: 0.8000\n",
      "Epoch 463/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4023 - accuracy: 0.9896\n",
      "Epoch 463: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4414 - accuracy: 0.9796 - val_loss: 0.8957 - val_accuracy: 0.8400\n",
      "Epoch 464/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4587 - accuracy: 0.9792\n",
      "Epoch 464: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4569 - accuracy: 0.9796 - val_loss: 0.8831 - val_accuracy: 0.8800\n",
      "Epoch 465/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4186 - accuracy: 0.9688\n",
      "Epoch 465: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4174 - accuracy: 0.9694 - val_loss: 0.9223 - val_accuracy: 0.8400\n",
      "Epoch 466/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4266 - accuracy: 0.9688\n",
      "Epoch 466: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4299 - accuracy: 0.9694 - val_loss: 0.9497 - val_accuracy: 0.7600\n",
      "Epoch 467/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4011 - accuracy: 0.9792\n",
      "Epoch 467: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.4001 - accuracy: 0.9796 - val_loss: 0.9480 - val_accuracy: 0.7600\n",
      "Epoch 468/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4701 - accuracy: 0.9583\n",
      "Epoch 468: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4682 - accuracy: 0.9592 - val_loss: 0.9130 - val_accuracy: 0.8400\n",
      "Epoch 469/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4410 - accuracy: 0.9688\n",
      "Epoch 469: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4392 - accuracy: 0.9694 - val_loss: 0.9296 - val_accuracy: 0.7200\n",
      "Epoch 470/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4935 - accuracy: 0.9583\n",
      "Epoch 470: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4907 - accuracy: 0.9592 - val_loss: 0.9506 - val_accuracy: 0.7200\n",
      "Epoch 471/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4196 - accuracy: 0.9583\n",
      "Epoch 471: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4193 - accuracy: 0.9592 - val_loss: 0.9536 - val_accuracy: 0.7200\n",
      "Epoch 472/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4035 - accuracy: 0.9896\n",
      "Epoch 472: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.4025 - accuracy: 0.9898 - val_loss: 0.9659 - val_accuracy: 0.7200\n",
      "Epoch 473/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.9898\n",
      "Epoch 473: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3820 - accuracy: 0.9898 - val_loss: 0.9559 - val_accuracy: 0.7200\n",
      "Epoch 474/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3975 - accuracy: 0.9792\n",
      "Epoch 474: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.3980 - accuracy: 0.9796 - val_loss: 0.9331 - val_accuracy: 0.7200\n",
      "Epoch 475/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.9694\n",
      "Epoch 475: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4293 - accuracy: 0.9694 - val_loss: 0.9595 - val_accuracy: 0.7200\n",
      "Epoch 476/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4757 - accuracy: 0.9583\n",
      "Epoch 476: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4729 - accuracy: 0.9592 - val_loss: 0.9445 - val_accuracy: 0.7200\n",
      "Epoch 477/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4916 - accuracy: 0.9583\n",
      "Epoch 477: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4896 - accuracy: 0.9592 - val_loss: 0.9030 - val_accuracy: 0.8000\n",
      "Epoch 478/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3939 - accuracy: 0.9688\n",
      "Epoch 478: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3951 - accuracy: 0.9694 - val_loss: 0.9149 - val_accuracy: 0.7200\n",
      "Epoch 479/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4249 - accuracy: 0.9583\n",
      "Epoch 479: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4231 - accuracy: 0.9592 - val_loss: 0.9491 - val_accuracy: 0.7200\n",
      "Epoch 480/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3820 - accuracy: 0.9896\n",
      "Epoch 480: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3825 - accuracy: 0.9898 - val_loss: 0.9899 - val_accuracy: 0.7200\n",
      "Epoch 481/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.9796\n",
      "Epoch 481: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4014 - accuracy: 0.9796 - val_loss: 0.8948 - val_accuracy: 0.8400\n",
      "Epoch 482/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3831 - accuracy: 0.9688\n",
      "Epoch 482: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3824 - accuracy: 0.9694 - val_loss: 0.9094 - val_accuracy: 0.8000\n",
      "Epoch 483/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3754 - accuracy: 1.0000\n",
      "Epoch 483: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.9115 - val_accuracy: 0.8000\n",
      "Epoch 484/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3789 - accuracy: 0.9896\n",
      "Epoch 484: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3787 - accuracy: 0.9898 - val_loss: 0.9548 - val_accuracy: 0.6800\n",
      "Epoch 485/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3882 - accuracy: 0.9896\n",
      "Epoch 485: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3879 - accuracy: 0.9898 - val_loss: 0.9347 - val_accuracy: 0.7200\n",
      "Epoch 486/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4257 - accuracy: 0.9583\n",
      "Epoch 486: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4242 - accuracy: 0.9592 - val_loss: 0.9450 - val_accuracy: 0.6800\n",
      "Epoch 487/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4054 - accuracy: 0.9583\n",
      "Epoch 487: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4049 - accuracy: 0.9592 - val_loss: 0.9277 - val_accuracy: 0.7200\n",
      "Epoch 488/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4173 - accuracy: 0.9688\n",
      "Epoch 488: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4158 - accuracy: 0.9694 - val_loss: 0.9636 - val_accuracy: 0.6400\n",
      "Epoch 489/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3858 - accuracy: 0.9896\n",
      "Epoch 489: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3859 - accuracy: 0.9898 - val_loss: 0.9112 - val_accuracy: 0.7600\n",
      "Epoch 490/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.9694\n",
      "Epoch 490: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4423 - accuracy: 0.9694 - val_loss: 0.9631 - val_accuracy: 0.7200\n",
      "Epoch 491/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3793 - accuracy: 0.9688\n",
      "Epoch 491: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3808 - accuracy: 0.9694 - val_loss: 0.9061 - val_accuracy: 0.8400\n",
      "Epoch 492/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4353 - accuracy: 0.9583\n",
      "Epoch 492: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4331 - accuracy: 0.9592 - val_loss: 0.8856 - val_accuracy: 0.7600\n",
      "Epoch 493/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5192 - accuracy: 0.9479\n",
      "Epoch 493: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5152 - accuracy: 0.9490 - val_loss: 0.9202 - val_accuracy: 0.8000\n",
      "Epoch 494/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4024 - accuracy: 0.9688\n",
      "Epoch 494: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.4007 - accuracy: 0.9694 - val_loss: 0.9070 - val_accuracy: 0.8000\n",
      "Epoch 495/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4372 - accuracy: 0.9583\n",
      "Epoch 495: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4855 - accuracy: 0.9490 - val_loss: 0.9728 - val_accuracy: 0.6400\n",
      "Epoch 496/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3947 - accuracy: 0.9583\n",
      "Epoch 496: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3932 - accuracy: 0.9592 - val_loss: 0.9677 - val_accuracy: 0.6400\n",
      "Epoch 497/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3976 - accuracy: 0.9792\n",
      "Epoch 497: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3959 - accuracy: 0.9796 - val_loss: 0.9696 - val_accuracy: 0.6800\n",
      "Epoch 498/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3826 - accuracy: 0.9688\n",
      "Epoch 498: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.3812 - accuracy: 0.9694 - val_loss: 0.9287 - val_accuracy: 0.8000\n",
      "Epoch 499/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3668 - accuracy: 0.9792\n",
      "Epoch 499: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3658 - accuracy: 0.9796 - val_loss: 0.9306 - val_accuracy: 0.7200\n",
      "Epoch 500/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3566 - accuracy: 0.9896\n",
      "Epoch 500: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3558 - accuracy: 0.9898 - val_loss: 0.9143 - val_accuracy: 0.7600\n",
      "Epoch 501/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3536 - accuracy: 0.9896\n",
      "Epoch 501: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.3529 - accuracy: 0.9898 - val_loss: 0.9282 - val_accuracy: 0.7600\n",
      "Epoch 502/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4214 - accuracy: 0.9792\n",
      "Epoch 502: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4218 - accuracy: 0.9796 - val_loss: 0.9340 - val_accuracy: 0.7200\n",
      "Epoch 503/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3610 - accuracy: 0.9896\n",
      "Epoch 503: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3602 - accuracy: 0.9898 - val_loss: 0.9488 - val_accuracy: 0.6800\n",
      "Epoch 504/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3615 - accuracy: 1.0000\n",
      "Epoch 504: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3604 - accuracy: 1.0000 - val_loss: 0.9212 - val_accuracy: 0.7200\n",
      "Epoch 505/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3838 - accuracy: 0.9688\n",
      "Epoch 505: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3824 - accuracy: 0.9694 - val_loss: 0.9036 - val_accuracy: 0.7600\n",
      "Epoch 506/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4305 - accuracy: 0.9688\n",
      "Epoch 506: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4307 - accuracy: 0.9694 - val_loss: 0.9455 - val_accuracy: 0.7200\n",
      "Epoch 507/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3468 - accuracy: 0.9896\n",
      "Epoch 507: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3460 - accuracy: 0.9898 - val_loss: 0.9327 - val_accuracy: 0.7200\n",
      "Epoch 508/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4242 - accuracy: 0.9583\n",
      "Epoch 508: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4219 - accuracy: 0.9592 - val_loss: 0.9417 - val_accuracy: 0.6800\n",
      "Epoch 509/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.9898\n",
      "Epoch 509: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.3387 - accuracy: 0.9898 - val_loss: 0.9677 - val_accuracy: 0.6800\n",
      "Epoch 510/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4167 - accuracy: 0.9688\n",
      "Epoch 510: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4144 - accuracy: 0.9694 - val_loss: 0.9763 - val_accuracy: 0.6800\n",
      "Epoch 511/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3451 - accuracy: 0.9896\n",
      "Epoch 511: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3447 - accuracy: 0.9898 - val_loss: 0.9939 - val_accuracy: 0.6400\n",
      "Epoch 512/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.9796\n",
      "Epoch 512: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3607 - accuracy: 0.9796 - val_loss: 0.9377 - val_accuracy: 0.7200\n",
      "Epoch 513/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4371 - accuracy: 0.9583\n",
      "Epoch 513: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4344 - accuracy: 0.9592 - val_loss: 0.9379 - val_accuracy: 0.7600\n",
      "Epoch 514/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4541 - accuracy: 0.9479\n",
      "Epoch 514: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4528 - accuracy: 0.9490 - val_loss: 0.9267 - val_accuracy: 0.8000\n",
      "Epoch 515/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3320 - accuracy: 0.9896\n",
      "Epoch 515: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3313 - accuracy: 0.9898 - val_loss: 0.9092 - val_accuracy: 0.8000\n",
      "Epoch 516/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3714 - accuracy: 0.9688\n",
      "Epoch 516: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3699 - accuracy: 0.9694 - val_loss: 0.9023 - val_accuracy: 0.7600\n",
      "Epoch 517/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.9796\n",
      "Epoch 517: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3612 - accuracy: 0.9796 - val_loss: 0.9052 - val_accuracy: 0.7200\n",
      "Epoch 518/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3499 - accuracy: 0.9896\n",
      "Epoch 518: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3489 - accuracy: 0.9898 - val_loss: 0.9048 - val_accuracy: 0.6800\n",
      "Epoch 519/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3993 - accuracy: 0.9688\n",
      "Epoch 519: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3983 - accuracy: 0.9694 - val_loss: 0.9574 - val_accuracy: 0.6400\n",
      "Epoch 520/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3447 - accuracy: 0.9688\n",
      "Epoch 520: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3436 - accuracy: 0.9694 - val_loss: 0.9118 - val_accuracy: 0.8000\n",
      "Epoch 521/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3454 - accuracy: 0.9896\n",
      "Epoch 521: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3551 - accuracy: 0.9796 - val_loss: 0.8986 - val_accuracy: 0.8000\n",
      "Epoch 522/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3618 - accuracy: 0.9792\n",
      "Epoch 522: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3607 - accuracy: 0.9796 - val_loss: 0.9009 - val_accuracy: 0.7600\n",
      "Epoch 523/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3159 - accuracy: 1.0000\n",
      "Epoch 523: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.9100 - val_accuracy: 0.7600\n",
      "Epoch 524/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3656 - accuracy: 0.9688\n",
      "Epoch 524: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3644 - accuracy: 0.9694 - val_loss: 0.9031 - val_accuracy: 0.7600\n",
      "Epoch 525/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3162 - accuracy: 1.0000\n",
      "Epoch 525: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.8975 - val_accuracy: 0.6800\n",
      "Epoch 526/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3298 - accuracy: 1.0000\n",
      "Epoch 526: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3457 - accuracy: 0.9898 - val_loss: 0.8624 - val_accuracy: 0.8400\n",
      "Epoch 527/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3635 - accuracy: 0.9688\n",
      "Epoch 527: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3618 - accuracy: 0.9694 - val_loss: 0.8683 - val_accuracy: 0.7600\n",
      "Epoch 528/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3948 - accuracy: 0.9792\n",
      "Epoch 528: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3927 - accuracy: 0.9796 - val_loss: 0.8925 - val_accuracy: 0.6800\n",
      "Epoch 529/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.9592\n",
      "Epoch 529: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3571 - accuracy: 0.9592 - val_loss: 0.8477 - val_accuracy: 0.8000\n",
      "Epoch 530/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3709 - accuracy: 0.9688\n",
      "Epoch 530: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3719 - accuracy: 0.9694 - val_loss: 0.9131 - val_accuracy: 0.6800\n",
      "Epoch 531/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3814 - accuracy: 0.9583\n",
      "Epoch 531: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3805 - accuracy: 0.9592 - val_loss: 0.8899 - val_accuracy: 0.7600\n",
      "Epoch 532/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3182 - accuracy: 0.9896\n",
      "Epoch 532: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3175 - accuracy: 0.9898 - val_loss: 0.9231 - val_accuracy: 0.6800\n",
      "Epoch 533/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3442 - accuracy: 0.9792\n",
      "Epoch 533: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3490 - accuracy: 0.9796 - val_loss: 0.9005 - val_accuracy: 0.7200\n",
      "Epoch 534/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3373 - accuracy: 0.9792\n",
      "Epoch 534: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3428 - accuracy: 0.9796 - val_loss: 0.8766 - val_accuracy: 0.8000\n",
      "Epoch 535/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3184 - accuracy: 1.0000\n",
      "Epoch 535: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.8721 - val_accuracy: 0.8000\n",
      "Epoch 536/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4245 - accuracy: 0.9479\n",
      "Epoch 536: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4252 - accuracy: 0.9490 - val_loss: 0.9076 - val_accuracy: 0.8400\n",
      "Epoch 537/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4051 - accuracy: 0.9583\n",
      "Epoch 537: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4116 - accuracy: 0.9490 - val_loss: 0.9719 - val_accuracy: 0.6800\n",
      "Epoch 538/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3094 - accuracy: 1.0000\n",
      "Epoch 538: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3087 - accuracy: 1.0000 - val_loss: 0.9667 - val_accuracy: 0.6800\n",
      "Epoch 539/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3056 - accuracy: 1.0000\n",
      "Epoch 539: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3050 - accuracy: 1.0000 - val_loss: 0.9513 - val_accuracy: 0.7600\n",
      "Epoch 540/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2889 - accuracy: 1.0000\n",
      "Epoch 540: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2898 - accuracy: 1.0000 - val_loss: 0.9295 - val_accuracy: 0.6800\n",
      "Epoch 541/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3220 - accuracy: 1.0000\n",
      "Epoch 541: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.8961 - val_accuracy: 0.8000\n",
      "Epoch 542/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3752 - accuracy: 0.9583\n",
      "Epoch 542: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3733 - accuracy: 0.9592 - val_loss: 0.9125 - val_accuracy: 0.7200\n",
      "Epoch 543/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.9898\n",
      "Epoch 543: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3115 - accuracy: 0.9898 - val_loss: 0.9236 - val_accuracy: 0.7200\n",
      "Epoch 544/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3411 - accuracy: 0.9792\n",
      "Epoch 544: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3397 - accuracy: 0.9796 - val_loss: 0.9061 - val_accuracy: 0.7200\n",
      "Epoch 545/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3112 - accuracy: 0.9896\n",
      "Epoch 545: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3107 - accuracy: 0.9898 - val_loss: 0.9029 - val_accuracy: 0.7600\n",
      "Epoch 546/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3349 - accuracy: 0.9792\n",
      "Epoch 546: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3337 - accuracy: 0.9796 - val_loss: 0.8733 - val_accuracy: 0.8000\n",
      "Epoch 547/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3734 - accuracy: 0.9688\n",
      "Epoch 547: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3714 - accuracy: 0.9694 - val_loss: 0.8795 - val_accuracy: 0.7600\n",
      "Epoch 548/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3194 - accuracy: 0.9896\n",
      "Epoch 548: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3186 - accuracy: 0.9898 - val_loss: 0.8838 - val_accuracy: 0.8000\n",
      "Epoch 549/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3324 - accuracy: 0.9792\n",
      "Epoch 549: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3313 - accuracy: 0.9796 - val_loss: 0.8888 - val_accuracy: 0.8000\n",
      "Epoch 550/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3424 - accuracy: 0.9688\n",
      "Epoch 550: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3424 - accuracy: 0.9694 - val_loss: 0.9239 - val_accuracy: 0.7600\n",
      "Epoch 551/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3952 - accuracy: 0.9479\n",
      "Epoch 551: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3954 - accuracy: 0.9490 - val_loss: 0.9326 - val_accuracy: 0.7200\n",
      "Epoch 552/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3237 - accuracy: 0.9792\n",
      "Epoch 552: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3225 - accuracy: 0.9796 - val_loss: 0.9176 - val_accuracy: 0.7200\n",
      "Epoch 553/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3139 - accuracy: 0.9688\n",
      "Epoch 553: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3131 - accuracy: 0.9694 - val_loss: 0.9215 - val_accuracy: 0.7600\n",
      "Epoch 554/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3125 - accuracy: 1.0000\n",
      "Epoch 554: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.9408 - val_accuracy: 0.7600\n",
      "Epoch 555/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2936 - accuracy: 1.0000\n",
      "Epoch 555: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2956 - accuracy: 1.0000 - val_loss: 0.9219 - val_accuracy: 0.8000\n",
      "Epoch 556/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3648 - accuracy: 0.9479\n",
      "Epoch 556: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3628 - accuracy: 0.9490 - val_loss: 0.9403 - val_accuracy: 0.6800\n",
      "Epoch 557/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3123 - accuracy: 0.9896\n",
      "Epoch 557: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3136 - accuracy: 0.9898 - val_loss: 0.9627 - val_accuracy: 0.6000\n",
      "Epoch 558/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2897 - accuracy: 1.0000\n",
      "Epoch 558: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2892 - accuracy: 1.0000 - val_loss: 0.9382 - val_accuracy: 0.6800\n",
      "Epoch 559/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2968 - accuracy: 0.9896\n",
      "Epoch 559: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3075 - accuracy: 0.9796 - val_loss: 0.9067 - val_accuracy: 0.7200\n",
      "Epoch 560/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3754 - accuracy: 0.9792\n",
      "Epoch 560: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3739 - accuracy: 0.9796 - val_loss: 0.9368 - val_accuracy: 0.6800\n",
      "Epoch 561/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3471 - accuracy: 0.9792\n",
      "Epoch 561: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3456 - accuracy: 0.9796 - val_loss: 0.8838 - val_accuracy: 0.6800\n",
      "Epoch 562/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3645 - accuracy: 0.9583\n",
      "Epoch 562: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3623 - accuracy: 0.9592 - val_loss: 0.9034 - val_accuracy: 0.7200\n",
      "Epoch 563/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3037 - accuracy: 0.9896\n",
      "Epoch 563: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3169 - accuracy: 0.9796 - val_loss: 0.8516 - val_accuracy: 0.8400\n",
      "Epoch 564/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3607 - accuracy: 0.9583\n",
      "Epoch 564: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3596 - accuracy: 0.9592 - val_loss: 0.8992 - val_accuracy: 0.7600\n",
      "Epoch 565/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2988 - accuracy: 0.9896\n",
      "Epoch 565: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2980 - accuracy: 0.9898 - val_loss: 0.8850 - val_accuracy: 0.8400\n",
      "Epoch 566/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3027 - accuracy: 1.0000\n",
      "Epoch 566: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3018 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 0.6800\n",
      "Epoch 567/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3224 - accuracy: 0.9688\n",
      "Epoch 567: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3210 - accuracy: 0.9694 - val_loss: 0.9159 - val_accuracy: 0.6800\n",
      "Epoch 568/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.9796\n",
      "Epoch 568: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3451 - accuracy: 0.9796 - val_loss: 0.8768 - val_accuracy: 0.7600\n",
      "Epoch 569/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2656 - accuracy: 1.0000\n",
      "Epoch 569: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2668 - accuracy: 1.0000 - val_loss: 0.8327 - val_accuracy: 0.8400\n",
      "Epoch 570/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3381 - accuracy: 0.9792\n",
      "Epoch 570: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3373 - accuracy: 0.9796 - val_loss: 0.8871 - val_accuracy: 0.6800\n",
      "Epoch 571/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3141 - accuracy: 0.9792\n",
      "Epoch 571: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3128 - accuracy: 0.9796 - val_loss: 0.8834 - val_accuracy: 0.7200\n",
      "Epoch 572/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9898\n",
      "Epoch 572: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2978 - accuracy: 0.9898 - val_loss: 0.9274 - val_accuracy: 0.6000\n",
      "Epoch 573/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.9898\n",
      "Epoch 573: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3007 - accuracy: 0.9898 - val_loss: 0.9086 - val_accuracy: 0.6000\n",
      "Epoch 574/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9796\n",
      "Epoch 574: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3138 - accuracy: 0.9796 - val_loss: 0.8425 - val_accuracy: 0.8000\n",
      "Epoch 575/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3172 - accuracy: 0.9792\n",
      "Epoch 575: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3163 - accuracy: 0.9796 - val_loss: 0.8474 - val_accuracy: 0.7600\n",
      "Epoch 576/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8980\n",
      "Epoch 576: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4355 - accuracy: 0.8980 - val_loss: 0.8895 - val_accuracy: 0.7600\n",
      "Epoch 577/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3219 - accuracy: 0.9896\n",
      "Epoch 577: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3207 - accuracy: 0.9898 - val_loss: 0.9100 - val_accuracy: 0.6400\n",
      "Epoch 578/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2791 - accuracy: 1.0000\n",
      "Epoch 578: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2791 - accuracy: 1.0000 - val_loss: 0.9112 - val_accuracy: 0.7600\n",
      "Epoch 579/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3401 - accuracy: 0.9688\n",
      "Epoch 579: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3387 - accuracy: 0.9694 - val_loss: 0.9163 - val_accuracy: 0.6400\n",
      "Epoch 580/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 1.0000\n",
      "Epoch 580: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2630 - accuracy: 1.0000 - val_loss: 0.9103 - val_accuracy: 0.6800\n",
      "Epoch 581/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3075 - accuracy: 0.9792\n",
      "Epoch 581: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3064 - accuracy: 0.9796 - val_loss: 0.9539 - val_accuracy: 0.6400\n",
      "Epoch 582/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3172 - accuracy: 0.9792\n",
      "Epoch 582: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3164 - accuracy: 0.9796 - val_loss: 0.8880 - val_accuracy: 0.7200\n",
      "Epoch 583/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2950 - accuracy: 0.9896\n",
      "Epoch 583: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2940 - accuracy: 0.9898 - val_loss: 0.8617 - val_accuracy: 0.8000\n",
      "Epoch 584/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3096 - accuracy: 0.9688\n",
      "Epoch 584: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3082 - accuracy: 0.9694 - val_loss: 0.9068 - val_accuracy: 0.6400\n",
      "Epoch 585/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3013 - accuracy: 0.9792\n",
      "Epoch 585: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3001 - accuracy: 0.9796 - val_loss: 0.9030 - val_accuracy: 0.6400\n",
      "Epoch 586/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2896 - accuracy: 0.9896\n",
      "Epoch 586: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2894 - accuracy: 0.9898 - val_loss: 0.8639 - val_accuracy: 0.7200\n",
      "Epoch 587/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3196 - accuracy: 0.9583\n",
      "Epoch 587: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3211 - accuracy: 0.9592 - val_loss: 0.8747 - val_accuracy: 0.7600\n",
      "Epoch 588/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3138 - accuracy: 0.9688\n",
      "Epoch 588: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3123 - accuracy: 0.9694 - val_loss: 0.9052 - val_accuracy: 0.6000\n",
      "Epoch 589/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2958 - accuracy: 0.9896\n",
      "Epoch 589: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2974 - accuracy: 0.9898 - val_loss: 0.8513 - val_accuracy: 0.8000\n",
      "Epoch 590/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2734 - accuracy: 1.0000\n",
      "Epoch 590: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2728 - accuracy: 1.0000 - val_loss: 0.8675 - val_accuracy: 0.8000\n",
      "Epoch 591/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2654 - accuracy: 1.0000\n",
      "Epoch 591: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2648 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.8400\n",
      "Epoch 592/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2678 - accuracy: 1.0000\n",
      "Epoch 592: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2672 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.8000\n",
      "Epoch 593/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3275 - accuracy: 0.9792\n",
      "Epoch 593: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.3257 - accuracy: 0.9796 - val_loss: 0.9042 - val_accuracy: 0.6800\n",
      "Epoch 594/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2614 - accuracy: 1.0000\n",
      "Epoch 594: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2612 - accuracy: 1.0000 - val_loss: 0.9020 - val_accuracy: 0.7200\n",
      "Epoch 595/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2952 - accuracy: 0.9896\n",
      "Epoch 595: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2944 - accuracy: 0.9898 - val_loss: 0.9162 - val_accuracy: 0.6000\n",
      "Epoch 596/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2717 - accuracy: 0.9792\n",
      "Epoch 596: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2710 - accuracy: 0.9796 - val_loss: 0.8990 - val_accuracy: 0.6400\n",
      "Epoch 597/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3113 - accuracy: 0.9792\n",
      "Epoch 597: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3112 - accuracy: 0.9796 - val_loss: 0.8977 - val_accuracy: 0.6400\n",
      "Epoch 598/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3007 - accuracy: 0.9896\n",
      "Epoch 598: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2997 - accuracy: 0.9898 - val_loss: 0.8919 - val_accuracy: 0.7200\n",
      "Epoch 599/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2516 - accuracy: 1.0000\n",
      "Epoch 599: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2602 - accuracy: 1.0000 - val_loss: 0.8639 - val_accuracy: 0.7600\n",
      "Epoch 600/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3363 - accuracy: 0.9479\n",
      "Epoch 600: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3346 - accuracy: 0.9490 - val_loss: 0.8466 - val_accuracy: 0.6400\n",
      "Epoch 601/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3633 - accuracy: 0.9688\n",
      "Epoch 601: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3775 - accuracy: 0.9592 - val_loss: 0.8106 - val_accuracy: 0.8800\n",
      "Epoch 602/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2834 - accuracy: 0.9792\n",
      "Epoch 602: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2823 - accuracy: 0.9796 - val_loss: 0.8270 - val_accuracy: 0.7200\n",
      "Epoch 603/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9796\n",
      "Epoch 603: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2913 - accuracy: 0.9796 - val_loss: 0.8437 - val_accuracy: 0.7200\n",
      "Epoch 604/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2698 - accuracy: 0.9792\n",
      "Epoch 604: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2703 - accuracy: 0.9796 - val_loss: 0.8420 - val_accuracy: 0.7600\n",
      "Epoch 605/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4051 - accuracy: 0.9583\n",
      "Epoch 605: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4016 - accuracy: 0.9592 - val_loss: 0.8200 - val_accuracy: 0.7600\n",
      "Epoch 606/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2669 - accuracy: 0.9896\n",
      "Epoch 606: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.2661 - accuracy: 0.9898 - val_loss: 0.8155 - val_accuracy: 0.7600\n",
      "Epoch 607/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3051 - accuracy: 0.9583\n",
      "Epoch 607: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.3059 - accuracy: 0.9592 - val_loss: 0.9042 - val_accuracy: 0.7200\n",
      "Epoch 608/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2752 - accuracy: 0.9896\n",
      "Epoch 608: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2751 - accuracy: 0.9898 - val_loss: 0.8553 - val_accuracy: 0.6800\n",
      "Epoch 609/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2865 - accuracy: 0.9896\n",
      "Epoch 609: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2939 - accuracy: 0.9796 - val_loss: 0.9449 - val_accuracy: 0.7200\n",
      "Epoch 610/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3172 - accuracy: 0.9688\n",
      "Epoch 610: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3154 - accuracy: 0.9694 - val_loss: 0.8716 - val_accuracy: 0.6800\n",
      "Epoch 611/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2673 - accuracy: 0.9792\n",
      "Epoch 611: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2789 - accuracy: 0.9796 - val_loss: 0.8728 - val_accuracy: 0.6400\n",
      "Epoch 612/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2712 - accuracy: 0.9896\n",
      "Epoch 612: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2711 - accuracy: 0.9898 - val_loss: 0.8433 - val_accuracy: 0.6400\n",
      "Epoch 613/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2800 - accuracy: 0.9792\n",
      "Epoch 613: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3009 - accuracy: 0.9694 - val_loss: 0.8400 - val_accuracy: 0.7600\n",
      "Epoch 614/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9694\n",
      "Epoch 614: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2860 - accuracy: 0.9694 - val_loss: 0.8156 - val_accuracy: 0.8000\n",
      "Epoch 615/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9898\n",
      "Epoch 615: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2691 - accuracy: 0.9898 - val_loss: 0.7993 - val_accuracy: 0.7200\n",
      "Epoch 616/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2716 - accuracy: 0.9792\n",
      "Epoch 616: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2709 - accuracy: 0.9796 - val_loss: 0.8339 - val_accuracy: 0.7200\n",
      "Epoch 617/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9898\n",
      "Epoch 617: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2789 - accuracy: 0.9898 - val_loss: 0.8740 - val_accuracy: 0.6400\n",
      "Epoch 618/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2897 - accuracy: 0.9792\n",
      "Epoch 618: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2912 - accuracy: 0.9796 - val_loss: 0.8484 - val_accuracy: 0.7200\n",
      "Epoch 619/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9796\n",
      "Epoch 619: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2909 - accuracy: 0.9796 - val_loss: 0.8343 - val_accuracy: 0.7600\n",
      "Epoch 620/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2612 - accuracy: 1.0000\n",
      "Epoch 620: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2604 - accuracy: 1.0000 - val_loss: 0.8393 - val_accuracy: 0.6400\n",
      "Epoch 621/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2490 - accuracy: 1.0000\n",
      "Epoch 621: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2485 - accuracy: 1.0000 - val_loss: 0.8157 - val_accuracy: 0.8000\n",
      "Epoch 622/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3270 - accuracy: 0.9688\n",
      "Epoch 622: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3248 - accuracy: 0.9694 - val_loss: 0.8093 - val_accuracy: 0.7600\n",
      "Epoch 623/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9796\n",
      "Epoch 623: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.2725 - accuracy: 0.9796 - val_loss: 0.7733 - val_accuracy: 0.8800\n",
      "Epoch 624/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2612 - accuracy: 0.9896\n",
      "Epoch 624: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2604 - accuracy: 0.9898 - val_loss: 0.7912 - val_accuracy: 0.8400\n",
      "Epoch 625/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3569 - accuracy: 0.9479\n",
      "Epoch 625: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3565 - accuracy: 0.9490 - val_loss: 0.8293 - val_accuracy: 0.8000\n",
      "Epoch 626/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 1.0000\n",
      "Epoch 626: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2676 - accuracy: 1.0000 - val_loss: 0.8139 - val_accuracy: 0.8000\n",
      "Epoch 627/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2670 - accuracy: 0.9792\n",
      "Epoch 627: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2662 - accuracy: 0.9796 - val_loss: 0.8511 - val_accuracy: 0.6800\n",
      "Epoch 628/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2823 - accuracy: 0.9792\n",
      "Epoch 628: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2813 - accuracy: 0.9796 - val_loss: 0.8347 - val_accuracy: 0.7200\n",
      "Epoch 629/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2814 - accuracy: 0.9792\n",
      "Epoch 629: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2801 - accuracy: 0.9796 - val_loss: 0.8164 - val_accuracy: 0.7600\n",
      "Epoch 630/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3605 - accuracy: 0.9688\n",
      "Epoch 630: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3873 - accuracy: 0.9490 - val_loss: 0.8661 - val_accuracy: 0.7600\n",
      "Epoch 631/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2613 - accuracy: 0.9896\n",
      "Epoch 631: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2646 - accuracy: 0.9898 - val_loss: 0.8279 - val_accuracy: 0.7200\n",
      "Epoch 632/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2941 - accuracy: 0.9583\n",
      "Epoch 632: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2925 - accuracy: 0.9592 - val_loss: 0.8075 - val_accuracy: 0.8800\n",
      "Epoch 633/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2891 - accuracy: 0.9688\n",
      "Epoch 633: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2878 - accuracy: 0.9694 - val_loss: 0.8018 - val_accuracy: 0.8400\n",
      "Epoch 634/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2590 - accuracy: 1.0000\n",
      "Epoch 634: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2582 - accuracy: 1.0000 - val_loss: 0.8185 - val_accuracy: 0.7600\n",
      "Epoch 635/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2448 - accuracy: 1.0000\n",
      "Epoch 635: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2442 - accuracy: 1.0000 - val_loss: 0.8341 - val_accuracy: 0.7200\n",
      "Epoch 636/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2611 - accuracy: 0.9896\n",
      "Epoch 636: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2602 - accuracy: 0.9898 - val_loss: 0.8671 - val_accuracy: 0.6400\n",
      "Epoch 637/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2387 - accuracy: 1.0000\n",
      "Epoch 637: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2384 - accuracy: 1.0000 - val_loss: 0.8453 - val_accuracy: 0.7200\n",
      "Epoch 638/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2587 - accuracy: 0.9792\n",
      "Epoch 638: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2611 - accuracy: 0.9796 - val_loss: 0.9251 - val_accuracy: 0.6400\n",
      "Epoch 639/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2674 - accuracy: 0.9792\n",
      "Epoch 639: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2786 - accuracy: 0.9694 - val_loss: 0.9111 - val_accuracy: 0.6800\n",
      "Epoch 640/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2498 - accuracy: 1.0000\n",
      "Epoch 640: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2491 - accuracy: 1.0000 - val_loss: 0.8780 - val_accuracy: 0.6800\n",
      "Epoch 641/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2698 - accuracy: 0.9792\n",
      "Epoch 641: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2757 - accuracy: 0.9796 - val_loss: 0.8659 - val_accuracy: 0.6800\n",
      "Epoch 642/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9592\n",
      "Epoch 642: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2700 - accuracy: 0.9592 - val_loss: 0.8428 - val_accuracy: 0.7600\n",
      "Epoch 643/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2806 - accuracy: 0.9688\n",
      "Epoch 643: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2912 - accuracy: 0.9592 - val_loss: 0.8022 - val_accuracy: 0.8400\n",
      "Epoch 644/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2541 - accuracy: 0.9896\n",
      "Epoch 644: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2702 - accuracy: 0.9796 - val_loss: 0.8155 - val_accuracy: 0.8000\n",
      "Epoch 645/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9898\n",
      "Epoch 645: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2705 - accuracy: 0.9898 - val_loss: 0.8123 - val_accuracy: 0.8400\n",
      "Epoch 646/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 1.0000\n",
      "Epoch 646: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2225 - accuracy: 1.0000 - val_loss: 0.8380 - val_accuracy: 0.8000\n",
      "Epoch 647/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3284 - accuracy: 0.9792\n",
      "Epoch 647: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3259 - accuracy: 0.9796 - val_loss: 0.8712 - val_accuracy: 0.7600\n",
      "Epoch 648/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2445 - accuracy: 0.9896\n",
      "Epoch 648: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2442 - accuracy: 0.9898 - val_loss: 0.8357 - val_accuracy: 0.6800\n",
      "Epoch 649/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2858 - accuracy: 0.9688\n",
      "Epoch 649: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2876 - accuracy: 0.9694 - val_loss: 0.8202 - val_accuracy: 0.7600\n",
      "Epoch 650/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2615 - accuracy: 0.9896\n",
      "Epoch 650: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2605 - accuracy: 0.9898 - val_loss: 0.8636 - val_accuracy: 0.7600\n",
      "Epoch 651/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3002 - accuracy: 0.9792\n",
      "Epoch 651: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2989 - accuracy: 0.9796 - val_loss: 0.8469 - val_accuracy: 0.7200\n",
      "Epoch 652/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2581 - accuracy: 0.9792\n",
      "Epoch 652: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2572 - accuracy: 0.9796 - val_loss: 0.8699 - val_accuracy: 0.7200\n",
      "Epoch 653/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2678 - accuracy: 0.9896\n",
      "Epoch 653: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2666 - accuracy: 0.9898 - val_loss: 0.8103 - val_accuracy: 0.8400\n",
      "Epoch 654/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 1.0000\n",
      "Epoch 654: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2357 - accuracy: 1.0000 - val_loss: 0.8907 - val_accuracy: 0.6800\n",
      "Epoch 655/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2454 - accuracy: 0.9896\n",
      "Epoch 655: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2448 - accuracy: 0.9898 - val_loss: 0.8669 - val_accuracy: 0.6800\n",
      "Epoch 656/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2737 - accuracy: 0.9792\n",
      "Epoch 656: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2724 - accuracy: 0.9796 - val_loss: 0.7990 - val_accuracy: 0.8400\n",
      "Epoch 657/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2459 - accuracy: 0.9896\n",
      "Epoch 657: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2453 - accuracy: 0.9898 - val_loss: 0.7967 - val_accuracy: 0.8400\n",
      "Epoch 658/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2729 - accuracy: 0.9792\n",
      "Epoch 658: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2716 - accuracy: 0.9796 - val_loss: 0.8697 - val_accuracy: 0.6800\n",
      "Epoch 659/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2649 - accuracy: 0.9792\n",
      "Epoch 659: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2638 - accuracy: 0.9796 - val_loss: 0.8483 - val_accuracy: 0.7200\n",
      "Epoch 660/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2894 - accuracy: 0.9792\n",
      "Epoch 660: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2878 - accuracy: 0.9796 - val_loss: 0.8477 - val_accuracy: 0.7600\n",
      "Epoch 661/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2529 - accuracy: 0.9792\n",
      "Epoch 661: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2521 - accuracy: 0.9796 - val_loss: 0.8679 - val_accuracy: 0.6800\n",
      "Epoch 662/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2329 - accuracy: 1.0000\n",
      "Epoch 662: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2323 - accuracy: 1.0000 - val_loss: 0.8427 - val_accuracy: 0.8000\n",
      "Epoch 663/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2279 - accuracy: 1.0000\n",
      "Epoch 663: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2275 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.6800\n",
      "Epoch 664/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9796\n",
      "Epoch 664: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2494 - accuracy: 0.9796 - val_loss: 0.8170 - val_accuracy: 0.8800\n",
      "Epoch 665/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2486 - accuracy: 0.9896\n",
      "Epoch 665: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2476 - accuracy: 0.9898 - val_loss: 0.8883 - val_accuracy: 0.7200\n",
      "Epoch 666/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2431 - accuracy: 0.9896\n",
      "Epoch 666: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2423 - accuracy: 0.9898 - val_loss: 0.8776 - val_accuracy: 0.7600\n",
      "Epoch 667/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2574 - accuracy: 0.9688\n",
      "Epoch 667: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2563 - accuracy: 0.9694 - val_loss: 0.8396 - val_accuracy: 0.8400\n",
      "Epoch 668/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2323 - accuracy: 1.0000\n",
      "Epoch 668: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2346 - accuracy: 1.0000 - val_loss: 0.7871 - val_accuracy: 0.8800\n",
      "Epoch 669/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2421 - accuracy: 0.9896\n",
      "Epoch 669: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2414 - accuracy: 0.9898 - val_loss: 0.8456 - val_accuracy: 0.7200\n",
      "Epoch 670/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2284 - accuracy: 1.0000\n",
      "Epoch 670: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2282 - accuracy: 1.0000 - val_loss: 0.8255 - val_accuracy: 0.8000\n",
      "Epoch 671/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2161 - accuracy: 1.0000\n",
      "Epoch 671: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2157 - accuracy: 1.0000 - val_loss: 0.8166 - val_accuracy: 0.8000\n",
      "Epoch 672/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2361 - accuracy: 1.0000\n",
      "Epoch 672: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2353 - accuracy: 1.0000 - val_loss: 0.8371 - val_accuracy: 0.8000\n",
      "Epoch 673/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2625 - accuracy: 0.9688\n",
      "Epoch 673: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2617 - accuracy: 0.9694 - val_loss: 0.8861 - val_accuracy: 0.6400\n",
      "Epoch 674/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2719 - accuracy: 0.9792\n",
      "Epoch 674: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2762 - accuracy: 0.9796 - val_loss: 0.8278 - val_accuracy: 0.7200\n",
      "Epoch 675/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2600 - accuracy: 0.9792\n",
      "Epoch 675: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2595 - accuracy: 0.9796 - val_loss: 0.8239 - val_accuracy: 0.7600\n",
      "Epoch 676/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2687 - accuracy: 0.9688\n",
      "Epoch 676: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2672 - accuracy: 0.9694 - val_loss: 0.8453 - val_accuracy: 0.8000\n",
      "Epoch 677/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2446 - accuracy: 0.9792\n",
      "Epoch 677: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2436 - accuracy: 0.9796 - val_loss: 0.8800 - val_accuracy: 0.6800\n",
      "Epoch 678/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2484 - accuracy: 0.9792\n",
      "Epoch 678: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2523 - accuracy: 0.9796 - val_loss: 0.8806 - val_accuracy: 0.8000\n",
      "Epoch 679/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2264 - accuracy: 0.9896\n",
      "Epoch 679: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2257 - accuracy: 0.9898 - val_loss: 0.8851 - val_accuracy: 0.6800\n",
      "Epoch 680/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2618 - accuracy: 0.9792\n",
      "Epoch 680: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2605 - accuracy: 0.9796 - val_loss: 0.9025 - val_accuracy: 0.6800\n",
      "Epoch 681/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2222 - accuracy: 1.0000\n",
      "Epoch 681: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2220 - accuracy: 1.0000 - val_loss: 0.8836 - val_accuracy: 0.6800\n",
      "Epoch 682/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2238 - accuracy: 0.9896\n",
      "Epoch 682: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2233 - accuracy: 0.9898 - val_loss: 0.8805 - val_accuracy: 0.6400\n",
      "Epoch 683/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2834 - accuracy: 0.9688\n",
      "Epoch 683: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.2855 - accuracy: 0.9694 - val_loss: 0.8751 - val_accuracy: 0.6800\n",
      "Epoch 684/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2678 - accuracy: 0.9583\n",
      "Epoch 684: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2663 - accuracy: 0.9592 - val_loss: 0.8629 - val_accuracy: 0.8400\n",
      "Epoch 685/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2187 - accuracy: 1.0000\n",
      "Epoch 685: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2181 - accuracy: 1.0000 - val_loss: 0.8623 - val_accuracy: 0.7200\n",
      "Epoch 686/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2628 - accuracy: 0.9792\n",
      "Epoch 686: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2614 - accuracy: 0.9796 - val_loss: 0.9732 - val_accuracy: 0.6000\n",
      "Epoch 687/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2738 - accuracy: 0.9688\n",
      "Epoch 687: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2750 - accuracy: 0.9694 - val_loss: 0.9387 - val_accuracy: 0.6800\n",
      "Epoch 688/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2910 - accuracy: 0.9583\n",
      "Epoch 688: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2889 - accuracy: 0.9592 - val_loss: 0.8757 - val_accuracy: 0.6400\n",
      "Epoch 689/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2337 - accuracy: 1.0000\n",
      "Epoch 689: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2390 - accuracy: 1.0000 - val_loss: 0.8342 - val_accuracy: 0.6800\n",
      "Epoch 690/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2743 - accuracy: 0.9583\n",
      "Epoch 690: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2725 - accuracy: 0.9592 - val_loss: 0.8078 - val_accuracy: 0.8800\n",
      "Epoch 691/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2509 - accuracy: 0.9688\n",
      "Epoch 691: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2496 - accuracy: 0.9694 - val_loss: 0.8279 - val_accuracy: 0.6800\n",
      "Epoch 692/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.1983 - accuracy: 1.0000\n",
      "Epoch 692: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.1981 - accuracy: 1.0000 - val_loss: 0.8204 - val_accuracy: 0.7200\n",
      "Epoch 693/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2268 - accuracy: 1.0000\n",
      "Epoch 693: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2259 - accuracy: 1.0000 - val_loss: 0.8064 - val_accuracy: 0.7600\n",
      "Epoch 694/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2022 - accuracy: 1.0000\n",
      "Epoch 694: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2040 - accuracy: 1.0000 - val_loss: 0.8538 - val_accuracy: 0.6400\n",
      "Epoch 695/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2420 - accuracy: 0.9792\n",
      "Epoch 695: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.2412 - accuracy: 0.9796 - val_loss: 0.8102 - val_accuracy: 0.8000\n",
      "Epoch 696/700\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 1.0000\n",
      "Epoch 696: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2036 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.6800\n",
      "Epoch 697/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2041 - accuracy: 1.0000\n",
      "Epoch 697: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2037 - accuracy: 1.0000 - val_loss: 0.8466 - val_accuracy: 0.7200\n",
      "Epoch 698/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2333 - accuracy: 0.9896\n",
      "Epoch 698: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.2323 - accuracy: 0.9898 - val_loss: 0.8531 - val_accuracy: 0.7200\n",
      "Epoch 699/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2622 - accuracy: 0.9792\n",
      "Epoch 699: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2611 - accuracy: 0.9796 - val_loss: 0.8259 - val_accuracy: 0.8000\n",
      "Epoch 700/700\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2042 - accuracy: 1.0000\n",
      "Epoch 700: val_accuracy did not improve from 0.88000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.2039 - accuracy: 1.0000 - val_loss: 0.7950 - val_accuracy: 0.8400\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7950 - accuracy: 0.8400\n",
      "Test accuracy: 0.8399999737739563\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 700\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"onehot.hdf5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "seqModel = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpoint])\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x113e7ce20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(seqModel.history.keys())\n",
    "xc         = range(num_epochs)\n",
    "\n",
    "plt.plot(xc, seqModel.history['loss'], label='train loss')\n",
    "plt.plot(xc, seqModel.history['val_loss'], label='validation loss')\n",
    "plt.plot(xc, seqModel.history['accuracy'], label='accuracy')\n",
    "plt.plot(xc, seqModel.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with custom_object_scope({'SeqSelfAttention': SeqSelfAttention}):\n",
    "    load_model = keras.models.load_model('onehot.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2064 - accuracy: 0.9388\n",
      "1/1 [==============================] - 0s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "load_model.evaluate(X_train, y_train)\n",
    "y_test_ordinal = np.argmax(y_test, axis=1)\n",
    "y_pred_prob = load_model.predict(X_test)\n",
    "y_pred_ordinal = np.argmax(y_pred_prob, axis=1)\n",
    "confusion = confusion_matrix(y_test_ordinal, y_pred_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[1 0 0 0 0 0]\n",
      " [0 5 0 0 0 0]\n",
      " [1 1 6 0 0 0]\n",
      " [0 0 1 2 0 0]\n",
      " [0 0 0 0 6 0]\n",
      " [0 0 0 0 0 2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(confusion, classes = np.unique(y_test_ordinal), title='Confusion matrix', normalize=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3adb7d5fef717c587deb8377a86ec7783da6fdece6d2a9408ba836e669f2be8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
